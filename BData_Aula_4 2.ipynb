{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Big Data - Aula 4 \n",
    "---\n",
    "\n",
    "# Aula Prática: Python for DataScientists - Pandas + Sklearn\n",
    "\n",
    "# Algoritmic Concepts Revision and Distributed Computing Challenges\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Pandas + SkLearn\n",
    "\n",
    "A Melhor forma é usar a linguagem para resolver um problema real. \n",
    "Vamos carregar dados de um *dataset* de preços de casas na califórnia para determinarmos: \n",
    "\n",
    "- Quantas casas existem abaixo de 200 000 Dollars \n",
    "- Quais os 2 locais onde as casas são mais baratas \n",
    "\n",
    "- Quais os melhores negócios (casas sub valorizadas) - Classificação/Regressão\n",
    "- Quanto valeria uma casa na zona x, para 4 pessoas e com 200 $m^2$ ?  - Regressão\n",
    "\n",
    "\n",
    "(Do conjunto de perguntas em cima, quais são perguntas de DataMining e quais são perguntas exploratórias (queries))\n",
    "\n",
    "<font color='forestgreen'>\n",
    "O dataset que vamos descarregar é do SkLearn, manipula-lo de forma a obter algumas análises exploratórias dos dados (pre-datamining). \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## DataSet de Casas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# Start by importing the modules\n",
    "\n",
    "from sklearn import datasets\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#import data to variable\n",
    "\n",
    "data = datasets.california_housing.fetch_california_housing()\n",
    "#show disorganized data\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#first, let's print a description\n",
    "print(data.DESCR)\n",
    "\n",
    "#what is this object type?\n",
    "print(type(data.DESCR))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O que podemos concluir da descrição das features?\n",
    "\n",
    "Que são médias locais."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Qual a classe dos componentes de data?\n",
    "\n",
    "print(type(data.target))\n",
    "print(type(data.data))\n",
    "\n",
    "data.feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dframe = pd.DataFrame(data=data.data, columns=data.feature_names)\n",
    "dframe.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "ou seja, obtemos algo semelhante a um \"excel\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# Quão esparsos são estes dados:\n",
    "\n",
    "plt.figure()\n",
    "plt.title(\"espalhamento geográfico dos dados\")\n",
    "plt.xlabel(\"Latitude\")\n",
    "plt.ylabel(\"Longitude\")\n",
    "plt.grid()\n",
    "plt.axis('equal')\n",
    "plt.plot(dframe.Longitude, dframe.Latitude,  '.', color=\"b\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/calif.png\" style=\"width:60%; margin-left:0px\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "### Mas faltam os dados dos preços..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(data.DESCR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#ah! está no \"target\":\n",
    "print(data.target)\n",
    "\n",
    "data.target.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(data.data.shape)\n",
    "print(data.target.shape)\n",
    "print(data.target.reshape(-1,1).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#porque o s valores estão em centenas de milhares de dollars, multiplicamos por 100 000\n",
    "\n",
    "new_data =np.concatenate((data.data, data.target.reshape(-1,1)*100000), axis=1)\n",
    "\n",
    "print(data.data.shape, new_data.shape)\n",
    "\n",
    "print(type(data.feature_names))\n",
    "#para adicionar um valor a uma lista podmeos usar .append ou <list> + []. Vamos usar a segunda:\n",
    "\n",
    "names = data.feature_names + [\"preços\"]\n",
    "print(names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dframe = pd.DataFrame(new_data, columns=names)\n",
    "\n",
    "dframe.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Exercícios usando a DataFrame\n",
    "\n",
    "Relembrando as perguntas de hoje:\n",
    "    \n",
    "- Quantas casas existem abaixo de 200 000 Dollars\n",
    "- Quais os 2 locais onde as casas são mais baratas\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quantas casas existem abaixo de 200 000 Dollars ?\n",
    "\n",
    "(Recordam-se do problema das 10 palavras mais usadas numa lista da Aula Passada? Como resolveriam com Pandas?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dframe[\"preços\"]\n",
    "#dframe.loc[dframe[\"preços\"]<200000,\"preços\"]\n",
    "\n",
    "#dframe.iloc[0:10,]\n",
    "\n",
    "\n",
    "a= [1,2,3]\n",
    "b = [4,5,6]\n",
    "[(i,j) for i,j in zip(a,b)]\n",
    "\n",
    "#X = [a and b and c for a,b,c in zip(list(dframe[\"preços\"]<200000), list(dframe[\"preços\"]>200), list(dframe[\"Population\"]>200))]\n",
    "#X\n",
    "\n",
    "#small_price_data = dframe.loc[,:]\n",
    "#small_price_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "small_price_data.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quais os 2 locais onde as casas são mais baratas?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "dframe.sort_values(\"preços\").iloc[0:2,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Já agora, qual a média do preço das casas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#dframe.agg([\"mean\"])[\"preços\"]\n",
    "\n",
    "#dframe.groupby(by=[\"HouseAge\", \"MedInc\"]).mean().head()\n",
    "\n",
    "X = dframe.groupby(by=[\"HouseAge\", \"MedInc\"])\n",
    "?X.filter()\n",
    "\n",
    "#dframe[(dframe[\"preços\"]<200000) & (dframe[\"Population\"]>20000)] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "# Mais informação para melhorar a aula da hoje (e fazer os TPCs):\n",
    "    \n",
    "- pandas: https://pandas.pydata.org/pandas-docs/stable/10min.html\n",
    "- sklearn: http://scikit-learn.org/stable/tutorial/index.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Algoritmic Concepts Revision and Distributed Computing Challenges\n",
    "\n",
    "## PARTE 2\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# 4) Graph Based Data - Inverted Indexing\n",
    "----\n",
    "\n",
    "An inverted index is a data structure that makes it easy, given a term, to find (pointers to) all the places where that term occurs.\n",
    "\n",
    "\n",
    "<img src=\"images/inv_index.png\" style=\"width:70%\" />\n",
    "\n",
    "\n",
    "### Vamos implementar um inverted index?\n",
    "\n",
    "Considerem o seguinte texto, dividido por pontos finais:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw = \"In statistics, maximum likelihood estimation(MLE) is a method of estimating the parameters of a statistical model given observations, by finding the parameter values that maximize the likelihood of making the observations given the parameters. MLE can be seen as a special case of the maximum a posteriori estimation(MAP) that assumes a uniform prior distribution of the parameters, or as a variant of the MAP that ignores the prior and which therefore is unregularized. The method of maximum likelihood corresponds to many well-known estimation methods in statistics. For example, one may be interested in the heights of adult female penguins, but is unable to measure the height of every single penguin in a population due to cost or time constraints. Assuming that the heights are normally distributed with some unknown mean and variance, the mean and variance can be estimated with MLE while only knowing the heights of some sample of the overall population. MLE would accomplish this by taking the mean and variance as parameters and finding particular parametric values that make the observed results the most probable given the model. In general, for a fixed set of data and underlying statistical model, the method of maximum likelihood selects the set of values of the model parameters that maximizes the likelihood function. Intuitively, this maximizes the 'agreement' of the selected model with the observed data, and for discrete random variables it indeed maximizes the probability of the observed data under the resulting distribution. Maximum likelihood estimation gives a unified approach to estimation, which is well-defined in the case of the normal distribution and many other problems.\"\n",
    "\n",
    "raw_list = raw.split(\".\")\n",
    "len(raw_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"{}.\".format(raw_list[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"{}.\".format(raw_list[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "i_index = {}\n",
    "i_index[\"in\"] = {0:1} #word in, appears in doc 0, 1 time\n",
    "i_index[\"likelihood\"] = {1:1} #word in, appears in doc 1, 1 time\n",
    "\n",
    "print(i_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trabalho de Casa 1\n",
    "\n",
    "1) Implement in python a function i_index_gen() that given as input a set of documents (an array of strings) returns an  inverted index based. Clean the text from punctuations chars or upper letters. \n",
    "\n",
    "2) Make a second function that returns the same i_index based on a pandas DataFrame without indexes\n",
    "\n",
    "3) Make a third function that returns the same i_index based on a pandas DataFrame using indexes\n",
    "\n",
    "4) Compare the performance for the three methods using ```%%time``` (you should increase your text example) and take your conclusions how lists, DataFrames (with or without indexes) manage searches\n",
    "\n",
    "\n",
    "**Don't forget to**\n",
    "- present all results in a python notebook;\n",
    "- Comment your code;\n",
    "\n",
    "Good Luck"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Example on the usage of %timeit\n",
    "import math\n",
    "%timeit [math.cos((i/2**i)**3) for i in range(1,10000)]\n",
    "print(\"----\")\n",
    "%timeit [(i)**3 for i in range(1,10000)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "print(\"ola\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "#### Problems in Big Data Scenario?\n",
    "\n",
    "---\n",
    "\n",
    "- Inverted indexing is fundamentally a very large search problem.\n",
    "- Again, problems arise when data does not fit in memory or a single CPU is too short.\n",
    "- How to deal with millions of files or a giant ```i_index```? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# 5) Matrix Multiplication\n",
    "\n",
    "Matrix multiplication is used for almost any machine learning model creation since most methods are nowadays stronlgy supported by linear algebra.\n",
    "\n",
    "For instance: \n",
    "\n",
    "- Distance measure;\n",
    "- Gradient Calculation;\n",
    "- Graph Calculation;\n",
    "\n",
    "Let's look an example in distance measure between x points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "A = np.round(np.random.random([4,4])*100)\n",
    "A = np.tril(A)\n",
    "\n",
    "print(A)\n",
    "b = np.ones([4,1])*2\n",
    "print(b)\n",
    "\n",
    "np.matmul(A,b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "---\n",
    "\n",
    "#### Problems in Big Data Scenario?\n",
    "\n",
    "---\n",
    "\n",
    "- If Initial Matrix too large for memory? Apply a row multiplication per node!?\n",
    "\n",
    "- If not even a single line is able to be fitted in memory? Notice that each input in the resulting vector depends entirely on a single line of the matrix\n",
    "\n",
    "<img src=\"./images/matmul.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## How do this applies to distance calculation?\n",
    "\n",
    "How to calculate the distance between this two points (a,b) in $R^N$?\n",
    "\n",
    "\n",
    "$$D(\\vec{a},\\vec{b}) = \\sqrt{(a_x^2 - b_x^2) + (a_y^2 - b_y^2) + ...} = \\sqrt{\\sum_i^N{\\Delta ab_i ^2}} $$\n",
    "\n",
    "This is the so called Norm2 distance. There are others like Norm1 (Manhathan distance) where $D(a,b) = \\sum_i^{N}{|\\Delta ab_i|}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider a matrix \n",
    "\n",
    "\n",
    "$$W =  \\begin{bmatrix}\n",
    "    2 & -1 \\\\\n",
    "    5 & 1 \\\\\n",
    "    6 & 9 \\\\\n",
    "    -2 & 3    \n",
    "\\end{bmatrix} = \\begin{bmatrix}\n",
    "    a \\\\\n",
    "    b \\\\\n",
    "    c \\\\\n",
    "    d \\\\    \n",
    "\\end{bmatrix}$$\n",
    "\n",
    "Let's compute the norm2 between each pair of points using normal programming techniques:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 2,  1],\n",
       "       [ 5,  1],\n",
       "       [ 6,  9],\n",
       "       [-2,  3]])"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np, math\n",
    "W = [[2, 1], [5,1], [6,9], [-2,3]]\n",
    "W = np.array(W)\n",
    "W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "NW = []\n",
    "for w1 in W:\n",
    "    NW.append([])\n",
    "    for w2 in W:\n",
    "        NW[-1].append(0)\n",
    "        for d in range(len(W[0])):\n",
    "            NW[-1][-1] += (w1[d]-w2[d])**2\n",
    "        NW[-1][-1] = math.sqrt(NW[-1][-1])\n",
    "        \n",
    "M = np.array(NW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  0.           3.           8.94427191   4.47213595]\n",
      " [  3.           0.           8.06225775   7.28010989]\n",
      " [  8.94427191   8.06225775   0.          10.        ]\n",
      " [  4.47213595   7.28010989  10.           0.        ]]\n",
      "----------------\n",
      "[[ 0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.]]\n"
     ]
    }
   ],
   "source": [
    "print(M)\n",
    "print(\"----------------\")\n",
    "print(M-M.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  0.        ,   3.        ,   8.94427191,   4.47213595],\n",
       "       [  3.        ,   0.        ,   8.06225775,   7.28010989],\n",
       "       [  8.94427191,   8.06225775,   0.        ,  10.        ],\n",
       "       [  4.47213595,   7.28010989,  10.        ,   0.        ]])"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "M = np.zeros((W.shape[0],W.shape[0]))\n",
    "\n",
    "for d in range(W.shape[1]):\n",
    "    w = np.repeat(W[:,d].reshape(-1,1),W.shape[0], axis=1)\n",
    "    M += (w-w.T)**2\n",
    "np.sqrt(M)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.69168128,  0.95795148],\n",
       "       [ 0.69728775,  0.58615352],\n",
       "       [ 0.03548373,  0.96888841],\n",
       "       ..., \n",
       "       [ 0.27724978,  0.92632415],\n",
       "       [ 0.23730736,  0.81649573],\n",
       "       [ 0.57104187,  0.10000069]])"
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W = np.random.random([10000,2])\n",
    "W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.          0.58875073  0.78891475 ...,  0.17017593  0.0838973\n",
      "   0.89904977]\n",
      " [ 0.58875073  0.          0.65226429 ...,  0.50255283  0.50536241\n",
      "   0.74941776]\n",
      " [ 0.78891475  0.65226429  0.         ...,  0.61887957  0.73582055\n",
      "   0.11302332]\n",
      " ..., \n",
      " [ 0.17017593  0.50255283  0.61887957 ...,  0.          0.12752133\n",
      "   0.72933304]\n",
      " [ 0.0838973   0.50536241  0.73582055 ...,  0.12752133  0.          0.84764047]\n",
      " [ 0.89904977  0.74941776  0.11302332 ...,  0.72933304  0.84764047  0.        ]]\n",
      "CPU times: user 2.92 s, sys: 9.23 ms, total: 2.93 s\n",
      "Wall time: 2.93 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "NW = []\n",
    "for w1 in W:\n",
    "    NW.append([])\n",
    "    for w2 in W:\n",
    "        NW[-1].append(0)\n",
    "        for d in range(len(W[0])):\n",
    "            NW[-1][-1] += (w1[d]-w2[d])**2\n",
    "        NW[-1][-1] = math.sqrt(NW[-1][-1])\n",
    "        \n",
    "M = np.array(NW)\n",
    "print(M)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.          0.37184023  0.65628869 ...,  0.41563657  0.47588379\n",
      "   0.86639103]\n",
      " [ 0.37184023  0.          0.76450674 ...,  0.54050713  0.51443123\n",
      "   0.5022774 ]\n",
      " [ 0.65628869  0.76450674  0.         ...,  0.2454843   0.25289584\n",
      "   1.02068035]\n",
      " ..., \n",
      " [ 0.41563657  0.54050713  0.2454843  ...,  0.          0.11686607\n",
      "   0.87699729]\n",
      " [ 0.47588379  0.51443123  0.25289584 ...,  0.11686607  0.          0.7904074 ]\n",
      " [ 0.86639103  0.5022774   1.02068035 ...,  0.87699729  0.7904074   0.        ]]\n",
      "CPU times: user 5.95 s, sys: 2.44 s, total: 8.38 s\n",
      "Wall time: 8.67 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "m = np.zeros((W.shape[0],W.shape[0]))\n",
    "\n",
    "for d in range(W.shape[1]):\n",
    "    w = np.repeat(W[:,d].reshape(-1,1),W.shape[0], axis=1)\n",
    "    m += (w-w.T)**2\n",
    "    \n",
    "M = np.sqrt(m)\n",
    "print(M)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "#W = np.random.random([1000,2])\n",
    "w = (np.repeat(W.reshape(-1,1,W.shape[1]),W.shape[0], axis=1)-np.repeat(W.reshape(1,-1,W.shape[1]),W.shape[0], axis=0))**2\n",
    "M = np.sum(w, axis=2)\n",
    "print(M)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "E este método como funciona?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Data Streamming (realtime data)\n",
    "\n",
    "This is a common situation in BigData that is not comparable with a normal data Scenario:\n",
    "\n",
    "**Data Streaming appears when we can think of the data as infinite and non-stationary (the distribution changes over time)**\n",
    "\n",
    "e.g. Google queries, Twitter or Facebook status updates\n",
    "\n",
    "### Characteristics:\n",
    "\n",
    "- The system cannot store the entire stream\n",
    "- Streams often deliver elements very rapidly. \n",
    "- We must process elements in real time, or we lose the opportunity to process them at all (forced to be main memory calculations)\n",
    "- ** Approximate solutions are often acceptable**\n",
    "\n",
    "\n",
    "\n",
    "### Kind of problems \n",
    "\n",
    "- Queries over sliding windows\n",
    "- Filtering a data stream\n",
    "- Select elements with property x from the stream\n",
    "- Counting distinct elements\n",
    "- Number of distinct elements in the last k elements of the stream\n",
    "- Estimating moments (average, Standard Deviation, etc.)\n",
    "- Finding frequent elements\n",
    "...\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Let's see 2 Examples\n",
    "\n",
    "### 1) Fixed Size Random Sampling (with substitution of new values over old values)\n",
    "\n",
    "Imagine you have a list of user queries with the following fields: \n",
    "\n",
    "Queries(UserID, QueryID, TimeStamp)\n",
    "\n",
    "Consider the following question: ***in the set of user queries, how many queries were used twice by the same user***? \n",
    "\n",
    "and consider you cannot keep all data: you have to random sampling your data.\n",
    "\n",
    "(hash functions are used for speed!)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Consider we have a d = 1000 (doubled queries) and x = 5000 (unique queries). So, a total of 7000 queries and 6000 unique queries (d+x).\n",
    "\n",
    "The right answer would be $1000/6000 = 16\\%$ (1000 queries of 6000 unique queries)\n",
    "\n",
    "Let us now random sample and see the results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 10366],\n",
       "       [110318],\n",
       "       [  2012],\n",
       "       ..., \n",
       "       [ 16337],\n",
       "       [ 90959],\n",
       "       [ 20869]])"
      ]
     },
     "execution_count": 219,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#generating data\n",
    "import math, random\n",
    "queries = np.random.choice(range(200000), 6000, replace=False)\n",
    "\n",
    "#create queryID, userID, TimeStamp\n",
    "queries = np.concatenate([queries, np.random.choice(queries,1000, replace=False)]).reshape(-1,1)\n",
    "np.random.shuffle(queries)\n",
    "queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7000, 1) (7000, 1) (7000, 1)\n",
      "[[     10366        119 1508513453]\n",
      " [    110318        126 1508514453]\n",
      " [      2012         53 1508515453]\n",
      " ..., \n",
      " [     16337        174 1515510453]\n",
      " [     90959        189 1515511453]\n",
      " [     20869         35 1515512453]]\n",
      "(679, 3)\n"
     ]
    }
   ],
   "source": [
    "#Add userId\n",
    "users = np.random.choice(range(200), 7000, replace=True).reshape(-1,1)\n",
    "\n",
    "#Add TimeStamp\n",
    "\n",
    "timestamp = np.array(range(1508513453,1515513453,1000)).reshape(-1,1)\n",
    "\n",
    "print(queries.shape, users.shape, timestamp.shape)\n",
    "data = np.concatenate([queries, users, timestamp], axis=1)\n",
    "#generating data OVER\n",
    "\n",
    "print(data)\n",
    "\n",
    "#Select 1/10 using an hash function:\n",
    "sub_queries = np.array([q for q in data if math.floor(random.random()*10)==1])\n",
    "print(sub_queries.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this technique do we have 1/10 of the data for sure? No, we have a noisy approximation of that quantity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample percentage\n",
      "0.04285714285714286 %\n",
      "-----\n",
      "Percentage of duplicates (should be 16%)\n",
      "0.14727540500736377 %\n"
     ]
    }
   ],
   "source": [
    "sub_queries.sort()\n",
    "X = np.diff(sub_queries[:,0])\n",
    "\n",
    "S = np.append((X==0), False)\n",
    "S\n",
    "\n",
    "\n",
    "print(\"sample percentage\")\n",
    "print(sub_queries.shape[1]/queries.shape[0]*100, \"%\")\n",
    "print(\"-----\")\n",
    "print(\"Percentage of duplicates (should be 16%)\")\n",
    "#print(sub_queries[S.tolist(),0].shape[0]/sub_queries.shape[0])\n",
    "print(sub_queries[S.tolist()].shape[0]/sub_queries.shape[0]*100, \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aprox. 10 times less.... why? Because we loose the duplicated proportion information. For aleatory values it can be shown that \n",
    "\n",
    "$$ Duplicates = d/(10s+19d)$$\n",
    "\n",
    "for 0.1 times sample. In this case, 1000/(60000+19000) = 1,2% (aprox 10% of the real value 16%) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conclusion:\n",
    "\n",
    "For many statistical queries, sampling destroys data is not well thought. In this case, we should consider other kind of information: for instance, the user: It's preferable to keep a complete query history of less users to have a good estimate of duplicated queries.\n",
    "\n",
    "**In that case, the hashing function should be done over the userID instead of the queryID**\n",
    "\n",
    "\n",
    "This allows to have related queries in the sample, and avoid relevant missing duplicates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) Filtering a Data Stream based on a given Set\n",
    "\n",
    "#### Section 1) made us have a second problem: Filtering a data stream based on a given Set of keys.\n",
    "\n",
    "If the selection criterion is a property of the regiter that can be calculated (e.g., an attribute is less than 10), then the selection is easy to do. \n",
    "\n",
    "**The problem becomes harder when the criterion involves lookup for membership in a set (specially when it does not fit in main memory).**\n",
    "\n",
    "\n",
    "----\n",
    "\n",
    "#### Example: Email Filtering based on \"from\" field using ***one gigabyte memory***:\n",
    "\n",
    "We have a list of 1E9 email names that are acceptable (if the email has 20 bytes, 20TB of space are needed).\n",
    "\n",
    "Disk Access? or ...(first Attempt) apply an hash function in this way:\n",
    "\n",
    "\n",
    "> - Given a set of keys **S** **(valid emails)** that we want to filter\n",
    "> - Create a bit array **B** of **n bits** (1GB means 8E9 bits), initially all 0s\n",
    "> - Choose a hash function h with range **[0,n)**\n",
    "> - Hash each member of s ∈ S to one of n buckets, and set that bit to 1, i.e., B[h(s)]=1   \n",
    "\n",
    "You have now the filter Ready. Let's Start Sending emails to the filter:\n",
    "\n",
    "> - For each new email **a** in the stream, Hash it as h(a)\n",
    "> - **Output a if B[h(a)] == 1**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's Analyze sizes:\n",
    "\n",
    "- Number of known emails:\n",
    "\n",
    "> |S| = 1E9 emails\n",
    "\n",
    "- Number of bits in the filter:\n",
    "\n",
    "> |B| = 8 Gbits (Feasible)\n",
    "\n",
    "- Number of bits setted to 1 in the filter:\n",
    "\n",
    "> (1/8 of the bits are set to 1 if each email assigns a bit to one - actually the value is slightly less)\n",
    "\n",
    "- If the email address is in S, then it surely hashes to a bucket set to 1, so it always gets through **(no false negatives)**\n",
    "\n",
    "\n",
    "- If stream emails are equally spanned by all buckets using $h(a)$, about 1/8th of the addresses not in S get through to the output generating **false positives** (span emails that pass through)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, there are a better implementation for big data: Bloom Filter (Next Time)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Graph Based Algorithms\n",
    "\n",
    "## 1) Page Rank\n",
    "\n",
    "#### Basic Idea: Links as votes for Ranking pages\n",
    "\n",
    "Each link’s vote is proportional to the importance of its source page\n",
    "\n",
    "- If page $j$ with importance $r_j$ has $n$ out-links, each link gets $r_j / n_j$ votes\n",
    "- Page $j$’s own importance is the sum of the votes on its in-links (I) (A page is important if it is pointed to by other important pages)\n",
    "\n",
    "So, for a single page $j$, its importance $r_j$ is given by:\n",
    "\n",
    "$$r_j = \\sum_{i=1}^{I} \\frac{r_i}{n_i}$$\n",
    "\n",
    "---\n",
    "\n",
    "Take a first set of 3 nodes:\n",
    "\n",
    "<img src=\"images/page_rank.png\" style=\"width:30%\"/>\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Let's write the equations, starting in $y$:\n",
    "\n",
    "$$r_y =r_y/2+r_a/2 $$\n",
    "\n",
    "$$r_a =r_y/2+r_m$$ \n",
    "\n",
    "$$r_m =r_a /2$$\n",
    "\n",
    "Is this solvable? What is wrong? The underlying matrix is $Singular$. \n",
    "We need a fourth equation to give this a solution:\n",
    "\n",
    "$$r_m + r_a + r_y = 1$$\n",
    "---\n",
    "\n",
    "\n",
    "Let's build a matrix M where this equations are represented in the following order (both rows and columns) $[y, a, m]$\n",
    "Our system (Homogenous) is then \n",
    "\n",
    "\n",
    "$$ \n",
    "\\begin{bmatrix} \n",
    "1/2 & -1/2 & 0\\\\\n",
    "-1/2 & 1 & -1\\\\\n",
    "0 & -1/2 & 1\\\\\n",
    "\\end{bmatrix}\n",
    "\\quad\n",
    "\\begin{bmatrix} \n",
    "y\\\\\n",
    "a\\\\\n",
    "m\\\\\n",
    "\\end{bmatrix} = \n",
    "\\begin{bmatrix} \n",
    "0\\\\\n",
    "0\\\\\n",
    "0\\\\\n",
    "\\end{bmatrix}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.5 -0.5  0. ]\n",
      " [-0.5  1.  -1. ]\n",
      " [ 0.  -0.5  1. ]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 244,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "M = np.array([[0.5, -0.5, 0],\n",
    "[-0.5, 1, -1],\n",
    "[0, -0.5, 1]])\n",
    "print(M)\n",
    "\n",
    "np.linalg.det(M)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We have to add an extra relation between parameters: for instance, a normalization information their relative values like $ y+a+m = 1$, which means a = 1-y-m. Let's use this in the last equation, obtaining $(y+m-1)/2 + m = 0 <=> y/2 + 3/2m  = 1/2$.\n",
    "\n",
    "\n",
    "\n",
    "$$ \n",
    "\\begin{bmatrix} \n",
    "1/2 & -1/2 & 0\\\\\n",
    "-1/2 & 1 & -1\\\\\n",
    "1/2 & 0 & 3/2\\\\\n",
    "\\end{bmatrix}\n",
    "\\quad\n",
    "\\begin{bmatrix} \n",
    "y\\\\\n",
    "a\\\\\n",
    "m\\\\\n",
    "\\end{bmatrix} = \n",
    "\\begin{bmatrix} \n",
    "0\\\\\n",
    "0\\\\\n",
    "1/2\\\\\n",
    "\\end{bmatrix}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.4],\n",
       "       [ 0.4],\n",
       "       [ 0.2]])"
      ]
     },
     "execution_count": 245,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "M = np.array([[0.5, -0.5, 0],\n",
    "[-0.5, 1, -1],\n",
    "[1/2, 0, 3/2]])\n",
    "\n",
    "#np.linalg.inv(M)\n",
    "b= np.array([[0], [0], [1/2]])\n",
    "\n",
    "X = np.linalg.solve(M,b)\n",
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But now if M is huge?\n",
    "linalg uses gaussian elimination. this is not good for huge matrices (Big Data).\n",
    "\n",
    "Inverting huge matrices is a very relvant subject in general. Also here."
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
