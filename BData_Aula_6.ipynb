{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aula 6 - BigData Algorithms\n",
    "---\n",
    "\n",
    "## TODAY\n",
    "\n",
    "- DataStreaming - Filtering DataStreaming FROM A SET  - **Bloom Filtering**\n",
    "- Window Counting 1 Occurrences in an array of 1 and 0 - **DGIM Method**\n",
    "- Counting Distinct Occorrences - **Flajolet-Martin** or **HyperLogLog**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Data Streamming (realtime data)\n",
    "\n",
    "This is a common situation in BigData that is not comparable with a normal data Scenario:\n",
    "\n",
    "**Data Streaming appears when we can think of the data as infinite and non-stationary (the distribution changes over time)**\n",
    "\n",
    "e.g. Google queries, Twitter or Facebook status updates\n",
    "\n",
    "### Characteristics:\n",
    "\n",
    "- The system cannot store the entire stream\n",
    "- Streams often deliver elements very rapidly. \n",
    "- We must process elements in real time, or we lose the opportunity to process them at all (forced to be main memory calculations)\n",
    "- ** Approximate solutions are often acceptable**\n",
    "\n",
    "\n",
    "\n",
    "### Kind of problems \n",
    "\n",
    "- Queries over sliding windows\n",
    "- Filtering a data stream\n",
    "- Select elements with property x from the stream\n",
    "- Counting distinct elements\n",
    "- Number of distinct elements in the last k elements of the stream\n",
    "- Estimating moments (average, Standard Deviation, etc.)\n",
    "- Finding frequent elements\n",
    "...\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Let's see 2 Examples\n",
    "\n",
    "### 1) Fixed Size Random Sampling (with substitution of new values over old values)\n",
    "\n",
    "Imagine you have a list of user queries with the following fields: \n",
    "\n",
    "Queries(UserID, QueryID, TimeStamp)\n",
    "\n",
    "Consider the following question: ***in the set of user queries, how many queries were used twice by the same user***? \n",
    "\n",
    "and consider you cannot keep all data: you have to random sampling your data.\n",
    "\n",
    "(hash functions are used for speed!)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Consider we have a d = 1000 (doubled queries) and x = 5000 (unique queries). So, a total of 7000 queries and 6000 unique queries (d+x).\n",
    "\n",
    "The right answer would be $1000/6000 = 16\\%$ (1000 queries of 6000 unique queries)\n",
    "\n",
    "Let us now random sample and see the results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 10366],\n",
       "       [110318],\n",
       "       [  2012],\n",
       "       ..., \n",
       "       [ 16337],\n",
       "       [ 90959],\n",
       "       [ 20869]])"
      ]
     },
     "execution_count": 219,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#generating data\n",
    "import math, random\n",
    "queries = np.random.choice(range(200000), 6000, replace=False)\n",
    "\n",
    "#create queryID, userID, TimeStamp\n",
    "queries = np.concatenate([queries, np.random.choice(queries,1000, replace=False)]).reshape(-1,1)\n",
    "np.random.shuffle(queries)\n",
    "queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7000, 1) (7000, 1) (7000, 1)\n",
      "[[     10366        119 1508513453]\n",
      " [    110318        126 1508514453]\n",
      " [      2012         53 1508515453]\n",
      " ..., \n",
      " [     16337        174 1515510453]\n",
      " [     90959        189 1515511453]\n",
      " [     20869         35 1515512453]]\n",
      "(679, 3)\n"
     ]
    }
   ],
   "source": [
    "#Add userId\n",
    "users = np.random.choice(range(200), 7000, replace=True).reshape(-1,1)\n",
    "\n",
    "#Add TimeStamp\n",
    "\n",
    "timestamp = np.array(range(1508513453,1515513453,1000)).reshape(-1,1)\n",
    "\n",
    "print(queries.shape, users.shape, timestamp.shape)\n",
    "data = np.concatenate([queries, users, timestamp], axis=1)\n",
    "#generating data OVER\n",
    "\n",
    "print(data)\n",
    "\n",
    "#Select 1/10 using an hash function:\n",
    "sub_queries = np.array([q for q in data if math.floor(random.random()*10)==1])\n",
    "print(sub_queries.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this technique do we have 1/10 of the data for sure? No, we have a noisy approximation of that quantity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample percentage\n",
      "0.04285714285714286 %\n",
      "-----\n",
      "Percentage of duplicates (should be 16%)\n",
      "0.14727540500736377 %\n"
     ]
    }
   ],
   "source": [
    "sub_queries.sort()\n",
    "X = np.diff(sub_queries[:,0])\n",
    "\n",
    "S = np.append((X==0), False)\n",
    "S\n",
    "\n",
    "\n",
    "print(\"sample percentage\")\n",
    "print(sub_queries.shape[1]/queries.shape[0]*100, \"%\")\n",
    "print(\"-----\")\n",
    "print(\"Percentage of duplicates (should be 16%)\")\n",
    "#print(sub_queries[S.tolist(),0].shape[0]/sub_queries.shape[0])\n",
    "print(sub_queries[S.tolist()].shape[0]/sub_queries.shape[0]*100, \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aprox. 10 times less.... why? Because we loose the duplicated proportion information. For aleatory values it can be shown that \n",
    "\n",
    "$$ Duplicates = d/(10s+19d)$$\n",
    "\n",
    "for 0.1 times sample. In this case, 1000/(60000+19000) = 1,2% (aprox 10% of the real value 16%) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conclusion:\n",
    "\n",
    "For many statistical queries, sampling destroys data is not well thought. In this case, we should consider other kind of information: for instance, the user: It's preferable to keep a complete query history of less users to have a good estimate of duplicated queries.\n",
    "\n",
    "**In that case, the hashing function should be done over the userID instead of the queryID**\n",
    "\n",
    "\n",
    "This allows to have related queries in the sample, and avoid relevant missing duplicates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) Filtering a Data Stream based on a given Set\n",
    "\n",
    "#### Section 1) made us have a second problem: Filtering a data stream based on a given Set of keys.\n",
    "\n",
    "If the selection criterion is a property of the regiter that can be calculated (e.g., an attribute is less than 10), then the selection is easy to do. \n",
    "\n",
    "**The problem becomes harder when the criterion involves lookup for membership in a set (specially when it does not fit in main memory).**\n",
    "\n",
    "\n",
    "----\n",
    "\n",
    "#### Example: Email Filtering based on \"from\" field using ***one gigabyte memory***:\n",
    "\n",
    "We have a list of 1E9 email names that are acceptable (if the email has 20 bytes, 20TB of space are needed).\n",
    "\n",
    "Disk Access? or ...(first Attempt) apply an hash function in this way:\n",
    "\n",
    "\n",
    "> - Given a set of keys **S (valid emails)** that we want to filter\n",
    "> - Create a bit array **B  of  n bits** (1GB means 8E9 bits), initially all 0s\n",
    "> - Choose a hash function h with range **[0,n)**\n",
    "> - Hash each member of s ∈ S to one of n buckets, and set that bit to 1, i.e., B[h(s)]=1   \n",
    "\n",
    "You have now the filter Ready. Let's Start Sending emails to the filter:\n",
    "\n",
    "> - For each new email **a** in the stream, Hash it as h(a)\n",
    "> - **Output a if B[h(a)] == 1**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's Analyze sizes:\n",
    "\n",
    "- Number of known emails:\n",
    "\n",
    "> |S| = 1E9 emails\n",
    "\n",
    "- Number of bits in the filter:\n",
    "\n",
    "> |B| = 8 Gbits (Feasible)\n",
    "\n",
    "- Number of bits setted to 1 in the filter:\n",
    "\n",
    "> (1/8 of the bits are set to 1 if each email assigns a bit to one - actually the value is slightly less)\n",
    "\n",
    "- If the email address is in S, then it surely hashes to a bucket set to 1, so it always gets through **(no false negatives)**\n",
    "\n",
    "\n",
    "- If stream emails are equally spanned by all buckets using $h(a)$, about 1/8th of the addresses not in S get through to the output generating **false positives** (span emails that pass through)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, there are a better implementation for big data: Bloom Filter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BLOOM FILTERS\n",
    "\n",
    "## Question: Filtering a data stream by elements existing in other set\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## 1st Method: Hash table Method\n",
    "This is the \"obvious solution\" \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> - Given a set of keys **S (valid emails)** that we want to filter\n",
    "> - Create a bit array **B n bits** (1GB means 8E9 bits), initially all 0s\n",
    "> - Choose a hash function $h$ with range **[0,n)**\n",
    "> - Hash each member of s ∈ S to one of n buckets, and set that bit to 1, i.e., B[h(s)]=1   \n",
    "\n",
    "You have now the filter Ready. Let's Start Sending emails to the filter:\n",
    "\n",
    "> - For each new email **a** in the stream, Hash it as h(a)\n",
    "> - **Output a if B[h(a)] == 1**\n",
    "\n",
    "\n",
    "<img src=\"images/hash_filter.png\" style=\"width:60%\"/>\n",
    "\n",
    "\n",
    "### Hash Table Results\n",
    "\n",
    "**It creates false positives but no false negatives**\n",
    "\n",
    "- |S| = 1 billion email addresses \n",
    "- |B|= 1GB = 8 billion bits\n",
    "\n",
    "If the email address is in S, then it surely hashes to a bucket that has the big set to 1, so it always gets through (no false negatives)\n",
    "\n",
    "**Approximately 1/8 of the bits are set to 1, so about 1/8th of the addresses not in S get through to the output (false positives)**\n",
    "\n",
    "Actually, less than 1/8th, because more than one address might hash to the same bit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2nd Method: Bloom Filter\n",
    "\n",
    "*Method Description:*\n",
    "\n",
    "- **Use $k$ independent hash functions $h_1 ,..., h_k$**\n",
    "- $|S| = m$ (Valid email addresses)\n",
    "- $|B| = n$ (Hash Filter array)\n",
    "\n",
    "\n",
    "- INITIALIZE\n",
    "    - Set $B$ to all $0s$\n",
    "    - Hash each element $s∈S$ using each hash function hi, set $B[h_i(s)]=1$ (for each $i=1,..,k$)\n",
    "\n",
    "> Remember: There is just 1 B array!\n",
    "\n",
    "\n",
    "- RUNTIME\n",
    "    - When a stream element with key $x$ (new email!) arrives, If $B[h_i(x)]=1$ **for all i=1,...,k** - That is, if $x$ hashes to a bucket set to $1$ for every hash function $h_i(x)$ - then declare that $x$ is in $S$ \n",
    "    - Otherwise discard the element x\n",
    "---\n",
    "\n",
    "<img src=\"images/bloom_filter.png\" style=\"width:40%\"/>\n",
    "\n",
    "----\n",
    "\n",
    "\n",
    "#### Bloom Filter False Positive Analisys\n",
    "\n",
    "m= 1 US billion (S)\n",
    "n= 8 US billion (B)\n",
    "\n",
    "- k=1: (Hash function): 0.1175 (about 1/8th)\n",
    "- k=2: 0.0493\n",
    "- k=6: 0.0235 (Optimum)\n",
    "\n",
    "**Only 2% false positive instead of ~12% (Hash function)**\n",
    "\n",
    "#### Bloom Filter Wrap-up\n",
    "\n",
    "- No false negatives\n",
    "- Great for pre-processing before more expensive checks\n",
    "- **Only 2% false positive instead of ~12% (Hash function)**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1153097240490381004"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hash(\"nuno@adasd.pr\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "\n",
    "# DGIM METHOD\n",
    "\n",
    "## Counting bits in a Window \n",
    "\n",
    "---\n",
    "\n",
    "### Question: How many 1s are in the last k bits? where k ≤ N\n",
    "---\n",
    "\n",
    "- Fixed-size sample or Reservoir Sampling or Sliding window\n",
    "\n",
    "Obvious solution:\n",
    "Store the most recent N bits\n",
    "When new bit comes in, discard the N+1st bit\n",
    "\n",
    "<img src=\"images/1stsol.png\" style=\"width:50%\"/>\n",
    "\n",
    "### Real problem\n",
    "\n",
    "What if we cannot afford to store N bits?\n",
    "E.g., we’re processing 1 billion streams and\n",
    "N = 1 billion\n",
    "> You can not get an exact answer without storing the entire window\n",
    "\n",
    "---\n",
    "\n",
    "#### 1 solution: Uniformity assumption\n",
    "\n",
    "<img src=\"images/unif_assumption.png\" style=\"width:50%\"/>\n",
    "\n",
    "Maintain 2 counters:\n",
    "- S: number of 1s from the beginning of the stream \n",
    "- Z: number of 0s from the beginning of the stream\n",
    "\n",
    "How many 1s are in the last N bits? This could be found by.\n",
    "\n",
    "$$N\\frac{S}{s+Z}$$\n",
    "\n",
    "\n",
    "#### Lacking Uniformity Assumption\n",
    "\n",
    "---\n",
    "\n",
    "#### 2 solution: Solution that doesn’t (quite) work\n",
    "\n",
    "- Summarize exponentially increasing regions of the stream, looking backward\n",
    "- Drop small regions if they begin at the same point as a larger region\n",
    "\n",
    "\n",
    "\n",
    "<img src=\"images/2nd_sol.png\" style=\"width:70%\"/>\n",
    "\n",
    "\n",
    "- We can reconstruct the count of the last N bits, except we are not sure how many of the last 6 1s are included in the N\n",
    "\n",
    "#### GOOD THINKS vs BAD THINGS:\n",
    "\n",
    "- Error in count no greater than the number of 1s in the “unknown” area\n",
    "- But it could be that all the 1s are in the unknown area at the end - in that case, the error is unbounded!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DGIM Method [Datar, Gionis, Indyk, Motwani]\n",
    "\n",
    "Instead of summarizing fixed-length blocks, summarize blocks with specific number of 1s - **Let the block sizes (and so, the number of 1s in each block) increase exponentially**\n",
    "\n",
    "- Either one or two blocks with the same power-of-2 number of 1s\n",
    "- Buckets do not overlap in timestamps   \n",
    "- Buckets are sorted by size\n",
    "- Earlier blocks are not smaller than later blocks   \n",
    "- Buckets disappear when their end-time is > N time units in the past\n",
    "\n",
    "\n",
    "<img src=\"images/DGIM.png\" style=\"width:70%\"/>\n",
    "\n",
    "---\n",
    "\n",
    "#### What is saved\n",
    "\n",
    "- (A) The timestamp of its end ```[O(log N) bits]```\n",
    "- (B) The number of 1s between its beginning and end ```[O(log log N) bits]```\n",
    "\n",
    "> Why the timestamp uses O(log N) bits: each timestamp is at most at position N. This position is stored with O(log N) bits (MAX) - so, we need ```[O(log N)]``` bits to keep all timestamps.\n",
    "\n",
    "> Why O(log log N) bits: With quantity of 1s = N,  since the number of 1s grows with power of 2, we can keep all blocks with ```log N``` bits. But for each bucket we need ```log N``` bits to keep the quantity - so we need ```O[log(log(N))]``` bits in total to keep qantities.\n",
    "\n",
    "---\n",
    "\n",
    "#### What happens when a new value come in?\n",
    "\n",
    "Two possibilies:\n",
    "\n",
    "- If the current bit is 0: no other changes are needed\n",
    "\n",
    "- If the current bit is 1:\n",
    "    - (1) Create a new bucket of size 1, for just this bit\n",
    "> End timestamp = current time\n",
    "    - (2) If there are now three blocks of size 1, combine the oldest two into a bucket of size 2\n",
    "    - (3) If there are now three blocks of size 2, combine the oldest two into a bucket of size 4\n",
    "    - (4) And so on...\n",
    "\n",
    "\n",
    "\n",
    "<img src=\"images/DGIM2.png\" style=\"width:70%\"/>\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "#### How to estimate the number of 1s\n",
    "\n",
    "To estimate the number of 1s in the most recent k<N bits:\n",
    "\n",
    "- Sum the sizes of all blocks but the last - size means the number of 1s.\n",
    "- Add just half the size of the last bucket\n",
    "\n",
    "> Why half the size: because we do not know how many 1s of the last bucket are still within the wanted window, our best guess is half the quantity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Resulting error: < 50%\n",
    "\n",
    "The error worst case scenario is when all data in the last bucket is wronlgy counted. That means 50% of the last bucket was considered to be 1s and it was 0's. Since the quantity of 1s in the last bucket is (in the worst case) 50% of the existent 1s in the last k bits, our error is at most 50%.\n",
    "\n",
    "\n",
    "#### Not enough? Add more blocks per power-of-two value\n",
    "\n",
    "We can improve this result by increasing the number of blocks (r) per power-of-two. This means we have, for instance, r=4\n",
    "\n",
    "\n",
    "```\n",
    "    ....16-16-16-16-8-8-8-8-4-4-4-4-2-2-2-2-1-1-1-1\n",
    "``` \n",
    "\n",
    "\n",
    "**In this case, the error is O(1/r)**\n",
    "\n",
    "By picking r appropriately, we can tradeoff **between number of bits we store** and the **error bound**\n",
    "\n",
    "\n",
    "## OTHER USAGES FOR DGIM METHOD\n",
    "\n",
    "Can we handle the case where the stream is not bits, but integers, and we want the sum\n",
    "of the last k elements? Yes.\n",
    "\n",
    "Why should we? **For instance, we want to calculate the Avg. price of last k sales.**\n",
    "\n",
    "### If you know all have at most m bits:\n",
    "\n",
    "- Treat each bit of the m bits of each integer as a separate stream\n",
    "- Use DGIM to count 1s in each integer ($c_i$)\n",
    "- We can calculate the total sum using\n",
    "    \n",
    "$$\\sum_{i=0}^{m-1}c_i2^i$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "\n",
    "# FLAJOLET-MARTIN METHOD \n",
    "## Counting Distinct Occorrences \n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Usage:\n",
    "\n",
    "- How many different Web pages does each customer request in a week?\n",
    "- How many distinct products have we sold in the last week?\n",
    "\n",
    "#### Characteristics\n",
    "\n",
    "\n",
    "- Data stream consists of a universe of elements chosen from a set of size N\n",
    "- Maintain a count of the number of distinct elements seen so far\n",
    "\n",
    "\n",
    "#### Obvious Approach \n",
    "\n",
    "**keep an hash with the set of elements seen so far**\n",
    "\n",
    "#### What if we do not have space to maintain the set of elements seen so far?\n",
    "\n",
    "and we want to...\n",
    "\n",
    "- Estimate the count in an unbiased way\n",
    "- Accept that the count may have a little error, but limit the probability that the error is large\n",
    "\n",
    "\n",
    "### FLAJOLET-MARTIN METHOD \n",
    "\n",
    "- Pick a hash function h that **maps randomly** each of the N elements to at least $log_2N$ bits ( binary representation for instance!) \n",
    "- For each stream element ```a```, let r(a) be the number of trailing 0s in h(a).\n",
    "\n",
    "> say h(a) = 12, then 12 is 1100 in binary, so r(a) = 2\n",
    "    \n",
    "    \n",
    "- Record R = the maximum r(a) seen\n",
    "- In That Case, **the estimated number of distinct elements is equal to $2^R$**\n",
    "\n",
    "(WHAT???)\n",
    "\n",
    "#### How it works empirically:\n",
    "\n",
    "- $h(a)$ is a sequence of $log_2 N$ bits\n",
    "\n",
    "\n",
    "- What fraction of all \"a\" values have a tail of r zeros:\n",
    "> - About 50% of as hash to ***0\n",
    "> - About 25% of as hash to **00\n",
    "> - About 12,5% of as hash to *000\n",
    "\n",
    "\n",
    "That means \n",
    "\n",
    "- $2^{-r}$ fraction of all \"a\" values have a tail of r zeros:\n",
    "\n",
    "**So, if we saw the longest tail of r=2 (i.e., item hash ending *100) then we have probably seen about 4 distinct items so far**\n",
    "\n",
    "This means that, on average, it takes to hash about $2^r$ items before we see one with zero-suffix of length r;\n",
    "\n",
    "\n",
    "#### How it works formally:\n",
    "\n",
    "- What is the probability that a given h(a) ends in at least r zeros is $2^{-r}$?\n",
    "- h(a) hashes elements uniformly at random   \n",
    "- Probability that a random number ends in at least r zeros is $2^{-r}$\n",
    "\n",
    "Then, the probability of NOT seeing a tail of length r among m elements is\n",
    "\n",
    "\n",
    "<img src=\"images/FLAJOLET-MARTIN.png\" style=\"width:40%\"/>\n",
    "\n",
    "Note that \n",
    "\n",
    "![image.png](attachment:image.png)\n",
    "\n",
    "- if m >> $2^r$, then prob. tends to 0\n",
    "- if m << $2^r$, then prob. tends to 1\n",
    "\n",
    "> This means that $2^r$ will almost always be around m"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TPC\n",
    "\n",
    "Implement a Flajolet Martin Algorithm and Understand how it works;"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
