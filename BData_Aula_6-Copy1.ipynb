{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aula 6 - BigData Algorithms\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODAY\n",
    "\n",
    "- Filtering DataStreaming  - **Bloom Filtering**\n",
    "- Matrix vector Multiplication (e.g. Page Rank) - **Power Iteration**\n",
    "- Window Counting - **DGIM Method**\n",
    "- Counting Distinct Occorrences - **Flajolet-Martin**\n",
    "\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BLOOM FILTERS\n",
    "\n",
    "## Question: Filtering a data stream by elements existing in other set\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "## 1st Method: Hash table Method\n",
    "This is the \"obvious solution\" (given before!)\n",
    "\n",
    "\n",
    "\n",
    "> - Given a set of keys **S** **(valid emails)** that we want to filter\n",
    "> - Create a bit array **B** of **n bits** (1GB means 8E9 bits), initially all 0s\n",
    "> - Choose a hash function $h$ with range **[0,n)**\n",
    "> - Hash each member of s ∈ S to one of n buckets, and set that bit to 1, i.e., B[h(s)]=1   \n",
    "\n",
    "You have now the filter Ready. Let's Start Sending emails to the filter:\n",
    "\n",
    "> - For each new email **a** in the stream, Hash it as h(a)\n",
    "> - **Output a if B[h(a)] == 1**\n",
    "\n",
    "\n",
    "<img src=\"images/hash_filter.png\" style=\"width:60%\"/>\n",
    "\n",
    "\n",
    "### Hash Table Results\n",
    "\n",
    "**It creates false positives but no false negatives**\n",
    "\n",
    "- |S| = 1 billion email addresses \n",
    "- |B|= 1GB = 8 billion bits\n",
    "\n",
    "If the email address is in S, then it surely hashes to a bucket that has the big set to 1, so it always gets through (no false negatives)\n",
    "\n",
    "**Approximately 1/8 of the bits are set to 1, so about 1/8th of the addresses not in S get through to the output (false positives)**\n",
    "\n",
    "Actually, less than 1/8th, because more than one address might hash to the same bit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2nd Method: Bloom Filter\n",
    "\n",
    "**Method Description: **\n",
    "\n",
    "- Use $k$ independent hash functions $h_1 ,..., h_k$\n",
    "- $|S| = m$ (Valid email addresses)\n",
    "- $|B| = n$ (Hash Filter array)\n",
    "\n",
    "\n",
    "- INITIALIZE\n",
    "    - Set $B$ to all $0s$\n",
    "    - Hash each element $s∈S$ using each hash function hi, set $B[h_i(s)]=1$ (for each $i=1,..,k$)\n",
    "\n",
    "> Remember: There is just 1 B array!\n",
    "\n",
    "\n",
    "- RUNTIME\n",
    "    - When a stream element with key $x$ (new email!) arrives, If $B[h_i(x)]=1$ **for all i=1,...,k** - That is, if $x$ hashes to a bucket set to $1$ for every hash function $h_i(x)$ - then declare that $x$ is in $S$ \n",
    "    - Otherwise discard the element x\n",
    "---\n",
    "\n",
    "<img src=\"images/bloom_filter.png\" style=\"width:40%\"/>\n",
    "\n",
    "----\n",
    "\n",
    "\n",
    "#### Bloom Filter False Positive Analisys\n",
    "\n",
    "m= 1 US billion (S)\n",
    "n= 8 US billion (B)\n",
    "\n",
    "- k=1: (Hash function): 0.1175 (about 1/8th)\n",
    "- k=2: 0.0493\n",
    "- k=6: 0.0235 (Optimum)\n",
    "\n",
    "**Only 2% false positive instead of ~12% (Hash function)**\n",
    "\n",
    "#### Bloom Filter Wrap-up\n",
    "\n",
    "- No false negatives\n",
    "- Great for pre-processing before more expensive checks\n",
    "- **Only 2% false positive instead of ~12% (Hash function)**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# POWER ITERATION\n",
    "\n",
    "## Matrix vector Multiplication (e.g. Page Rank) - \n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we saw before, Gaussian elimination method works for small examples, but we need a better method for large web-size graphs\n",
    "\n",
    "\n",
    "So, for a single page $j$, its importance $r_j$ is given by:\n",
    "\n",
    "$$r_j = \\sum_{i=1}^{I} \\frac{r_i}{n_i}$$\n",
    "\n",
    "---\n",
    "\n",
    "Take a first set of 3 nodes:\n",
    "\n",
    "<img src=\"images/page_rank.png\" style=\"width:30%\"/>\n",
    "\n",
    "\n",
    "Could be written as \n",
    "\n",
    "$$r_y =r_y/2+r_a/2 $$\n",
    "\n",
    "$$r_a =r_y/2+r_m$$ \n",
    "\n",
    "$$r_m =r_a /2$$\n",
    "\n",
    "\n",
    "We have to add an extra relation between parameters: for instance, a normalization information their relative values like $ y+a+m = 1$, which means a = 1-y-m. \n",
    "\n",
    "\n",
    "\n",
    "$$ \n",
    "\\begin{bmatrix} \n",
    "1/2 & -1/2 & 0\\\\\n",
    "-1/2 & 1 & -1\\\\\n",
    "1/2 & 0 & 3/2\\\\\n",
    "\\end{bmatrix}\n",
    "\\quad\n",
    "\\begin{bmatrix} \n",
    "y\\\\\n",
    "a\\\\\n",
    "m\\\\\n",
    "\\end{bmatrix} = \n",
    "\\begin{bmatrix} \n",
    "0\\\\\n",
    "0\\\\\n",
    "1/2\\\\\n",
    "\\end{bmatrix}$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What about if this matrix is HUGE?\n",
    "\n",
    "To solve this problem let's move back again to the original formulation and consider the matrix $M$ with dimensions D by D where D = number of web pages and  where $M_{ij}$ is given by \n",
    "\n",
    "``` If i == j then ```  \n",
    "\n",
    "$ \\space\\space\\space M_{ij} = 1/d_i$\n",
    "\n",
    "``` else```\n",
    "\n",
    "$ \\space\\space\\space M_{ij} =  0$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where $d_i$ are the number of outbound links;\n",
    "\n",
    "This is exactly the right side of \n",
    "\n",
    "\n",
    "$$r_y =r_y/2+r_a/2 $$\n",
    "\n",
    "$$r_a =r_y/2+r_m$$ \n",
    "\n",
    "$$r_m =r_a /2$$\n",
    "\n",
    "This relations are called the **flow equations** and they could be written as \n",
    "\n",
    "$$r = Mr$$\n",
    "\n",
    "where $r = [r_a r_y r_m]^T$ which are the page ratings.\n",
    "\n",
    "Suppose page $i$ links to 3 pages, including $j$, we the flow eqution states that $r_j = \\sum_{i=1}^{I} \\frac{r_i}{n_i}$.\n",
    "\n",
    "\n",
    "<img src=\"images/flow_equation.png\" style=\"width:60%\"/>\n",
    "\n",
    "\n",
    "If $Mr = r$ this means $Mr = Ir = \\lambda r$\n",
    "\n",
    "This means $r$ is a so called eigenvector of $M$ \n",
    "Actually, since $||\\sum_i(M_{ij})|| < 1$ (due to normalization) and since $\\lambda=1$, then $r$ is the biggest eigenvalue of $M$.\n",
    "\n",
    "> $x$ is an eigenvector of $A$ if $Ax = \\lambda x$ where $\\lambda$ is a real constant \n",
    "\n",
    "----\n",
    "\n",
    "## Power iteration: \n",
    "### A simple iterative scheme to solve find the dominant eigenvector (r)\n",
    "\n",
    "- Suppose there are N web pages\n",
    "- Initialize: $r^{(0)} = [1/N,....,1/N]^T$\n",
    "\n",
    "> This means all rankings are considered equal at the begining.\n",
    "\n",
    "- Iterate: $r^{(t+1)} = M.r^{(t)}$\n",
    "  \n",
    "- Stop when $\\|r(t+1) – r(t)\\| < ε$\n",
    "\n",
    "----\n",
    "\n",
    "\n",
    "## How it Works?\n",
    "\n",
    "$r^{(2)} = Mr^{(1)} = M(Mr^{(0)}) = M^2r^{(1)}$\n",
    "\n",
    "Claim: $M^k r^{(1)} \\rightarrow r$\n",
    "\n",
    "\n",
    "### Proof:\n",
    "$r$ could be written using a linear combination of the M eigenvectors x_1,...x_n (all). That means:\n",
    "\n",
    "$$r^{(0)} = c_1 x_1 + c_2 x_2 + ... + c_n x_n$$\n",
    "\n",
    "then\n",
    "\n",
    "$$Mr^{(0)} = M(c_1 x_1 + c_2 x_2 + ... + c_n x_n) = c_1 \\lambda_1 x_1 + c_2 \\lambda_2 x_2 + ... + c_n \\lambda_n x_n$$\n",
    "\n",
    "finally\n",
    "\n",
    "$$M^k r^{(0)} = c_1(\\lambda_1^k x_1) + c_2(\\lambda_2^k x_2) + ... + c_n(\\lambda_n^k x_n)$$\n",
    "\n",
    "since all $\\lambda$s (eigenvalues) are $<1$ besides $\\lambda_1=1$ (the one we want), when $k -> \\inf$\n",
    "\n",
    "$$M^k r^{(0)} -> c_1(x_1)$$\n",
    "\n",
    "But since $Mr = r$ for the right $r$, then $c_1 = 1 $\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Random Web Surfer Interpretation\n",
    "\n",
    "\n",
    "Imagine a random web surfer\n",
    "\n",
    "- At any time $n$ surfer is on some page $i$ \n",
    "- At time $n+1$, the surfer follows an out-link from $i$ uniformly at random \n",
    "- Ends up on some page $j$ linked from $i$\n",
    "- Process repeats indefinitely\n",
    "\n",
    "\n",
    "$r_j = \\sum_{i=1}^{N} \\frac{r_i}{n_i}$\n",
    "\n",
    "Consider $p(n)$ as the probability distribution of the websurfer at time $n$\n",
    "\n",
    "**What is the probability of the web surfer to be in the page $j$ at time $n+1$?**\n",
    "\n",
    "<img src=\"images/page_rank_as_prob.png\" style=\"width:40%\"/>\n",
    "\n",
    "Since we are navigating from anywhere, it depends on the probabilities of being in $i_1$, $i_2$ or $i_3$ at the previous step (n), and then, at random ($1/d_i$) jump to this page. That is,\n",
    "\n",
    "$$p(n+1) = M p(n)$$\n",
    "\n",
    "Suppose now that we reach \n",
    "\n",
    "$$p(n+1) = M p(n) = p(n)$$\n",
    "\n",
    "Which means, the probability distribution is not altered anymore by new steps. Then $p(t)$ is said to be a **stationary distribution of a random walk**.\n",
    "\n",
    "### Some Remarks\n",
    "\n",
    "- If we solve $p(n)$, random web surfer probability map, we can find or original $r$ vector (rank vector for each page). \n",
    "- So, each time we multiply $M$ by $r^k$ ($Mr^k$) we are calculating a new $p(n+1)$. \n",
    "- Each row in M takes the role of probabiltiy distribution to go from previous positions to new positions at n+1 time.\n",
    "\n",
    "** This was Google problem to be solved back in 1998. ***\n",
    "\n",
    "Let us finally see some issues to define the final equation to be solved.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Issues to solve the Final Problem\n",
    "\n",
    "**2 problems when solving the random walking problem**\n",
    "\n",
    "(1) Some pages are **dead ends** (have no out-links)\n",
    ">    Random walk has “nowhere” to go to\n",
    ">  - this leaks the resulting ranks\n",
    "\n",
    "(2) **Spider traps** (all out-links are within the group)\n",
    ">  Random walked gets “stuck” in a trap\n",
    ">  - And eventually spider traps absorb all importance\n",
    "  \n",
    "<img src=\"images/page_rank_probs.png\" style=\"width:30%\"/>\n",
    "\n",
    "\n",
    "**Google Solved this With Random TelePorts - Some times we should just randomly jump out to any other page to avoid dead ends and spider traps**\n",
    "\n",
    "\n",
    "Consider this random jump is done with probability $1-\\beta$ and not done with probability $\\beta$. \n",
    "This means each line of M has not zeros it allways have some probability of jumping to an unexpected page!\n",
    "\n",
    "The new flow equation is now:\n",
    "\n",
    "$$r_j = \\beta \\sum_i \\frac{r_i}{d_i} + (1-\\beta)\\frac{1}{N}$$\n",
    "\n",
    "\n",
    "\n",
    "And the new Google Matrix A is:\n",
    "\n",
    "$$A = \\beta M + (1-\\beta) \\left[\\frac{1}{N} \\right]_{N \\times N}$$\n",
    "$$ $$\n",
    "<div style=\"text-align:center\">[Brin-Page, ‘98]</div>\n",
    "\n",
    "http://ilpubs.stanford.edu:8090/422/1/1999-66.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finally, applying to Page Rank Calculation\n",
    "\n",
    "Key step is the following matrix-vector multiplication:\n",
    "\n",
    "$$r_{new} =A r_{old}$$\n",
    "\n",
    "This is easy if we have enough main memory to hold $A, r_{old}, r_{new}$\n",
    "\n",
    "Say N = 1 billion pages   \n",
    "We need 4 bytes for each entry (typical...)\n",
    "\n",
    "- aprox 8GB for each vector, that's high but solvable..\n",
    "\n",
    "- BUT Matrix A has N^2 entries where $N = 10^9$ -->   $10^{18}$ is a large number (plus 4 Bytes per entry!)\n",
    "\n",
    "This is infeasible. The final page rank equation should be: \n",
    "\n",
    "$$r_{new} = \\beta M r_{old} + \\left[\\frac{1-\\beta}{N} \\right]_{N \\times 1}$$\n",
    "\n",
    "\n",
    "M is sparse, so everything is feasible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "\n",
    "# DGIM METHOD\n",
    "\n",
    "## Counting bits in a Window \n",
    "\n",
    "---\n",
    "\n",
    "### Question: How many 1s are in the last k bits? where k ≤ N\n",
    "---\n",
    "\n",
    "- Fixed-size sample or Reservoir Sampling or Sliding window\n",
    "\n",
    "Obvious solution:\n",
    "Store the most recent N bits\n",
    "When new bit comes in, discard the N+1st bit\n",
    "\n",
    "<img src=\"images/1stsol.png\" style=\"width:50%\"/>\n",
    "\n",
    "### Real problem\n",
    "\n",
    "What if we cannot afford to store N bits?\n",
    "E.g., we’re processing 1 billion streams and\n",
    "N = 1 billion\n",
    "> You can not get an exact answer without storing the entire window\n",
    "\n",
    "---\n",
    "\n",
    "#### 1 solution: Uniformity assumption\n",
    "\n",
    "<img src=\"images/unif_assumption.png\" style=\"width:50%\"/>\n",
    "\n",
    "Maintain 2 counters:\n",
    "- S: number of 1s from the beginning of the stream \n",
    "- Z: number of 0s from the beginning of the stream\n",
    "\n",
    "How many 1s are in the last N bits? This could be found by.\n",
    "\n",
    "$$N\\frac{S}{s+Z}$$\n",
    "\n",
    "\n",
    "#### Lacking Uniformity Assumption\n",
    "\n",
    "---\n",
    "\n",
    "#### 2 solution: Solution that doesn’t (quite) work\n",
    "\n",
    "- Summarize exponentially increasing regions of the stream, looking backward\n",
    "- Drop small regions if they begin at the same point as a larger region\n",
    "\n",
    "\n",
    "\n",
    "<img src=\"images/2nd_sol.png\" style=\"width:70%\"/>\n",
    "\n",
    "\n",
    "- We can reconstruct the count of the last N bits, except we are not sure how many of the last 6 1s are included in the N\n",
    "\n",
    "#### GOOD THINKS vs BAD THINGS:\n",
    "\n",
    "- Error in count no greater than the number of 1s in the “unknown” area\n",
    "- But it could be that all the 1s are in the unknown area at the end - in that case, the error is unbounded!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DGIM Method [Datar, Gionis, Indyk, Motwani]\n",
    "\n",
    "Instead of summarizing fixed-length blocks, summarize blocks with specific number of 1s - **Let the block sizes (and so, the number of 1s in each block) increase exponentially**\n",
    "\n",
    "- Either one or two blocks with the same power-of-2 number of 1s\n",
    "- Buckets do not overlap in timestamps   \n",
    "- Buckets are sorted by size\n",
    "- Earlier blocks are not smaller than later blocks   \n",
    "- Buckets disappear when their end-time is > N time units in the past\n",
    "\n",
    "\n",
    "<img src=\"images/DGIM.png\" style=\"width:70%\"/>\n",
    "\n",
    "---\n",
    "\n",
    "#### What is saved\n",
    "\n",
    "- (A) The timestamp of its end ```[O(log N) bits]```\n",
    "- (B) The number of 1s between its beginning and end ```[O(log log N) bits]```\n",
    "\n",
    "> Why the timestamp uses O(log N) bits: each timestamp is at most at position N. This position is stored with O(log N) bits (MAX) - so, we need ```[O(log N)]``` bits to keep all timestamps.\n",
    "\n",
    "> Why O(log log N) bits: With quantity of 1s = N,  since the number of 1s grows with power of 2, we can keep all blocks with ```log N``` bits. But for each bucket we need ```log N``` bits to keep the quantity - so we need ```O[log(log(N))]``` bits in total to keep qantities.\n",
    "\n",
    "---\n",
    "\n",
    "#### What happens when a new value come in?\n",
    "\n",
    "Two possibilies:\n",
    "\n",
    "- If the current bit is 0: no other changes are needed\n",
    "\n",
    "- If the current bit is 1:\n",
    "    - (1) Create a new bucket of size 1, for just this bit\n",
    "> End timestamp = current time\n",
    "    - (2) If there are now three blocks of size 1, combine the oldest two into a bucket of size 2\n",
    "    - (3) If there are now three blocks of size 2, combine the oldest two into a bucket of size 4\n",
    "    - (4) And so on...\n",
    "\n",
    "\n",
    "\n",
    "<img src=\"images/DGIM2.png\" style=\"width:70%\"/>\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "#### How to estimate the number of 1s\n",
    "\n",
    "To estimate the number of 1s in the most recent k<N bits:\n",
    "\n",
    "- Sum the sizes of all blocks but the last - size means the number of 1s.\n",
    "- Add just half the size of the last bucket\n",
    "\n",
    "> Why half the size: because we do not know how many 1s of the last bucket are still within the wanted window, our best guess is half the quantity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Resulting error: < 50%\n",
    "\n",
    "The error worst case scenario is when all data in the last bucket is wronlgy counted. That means 50% of the last bucket was considered to be 1s and it was 0's. Since the quantity of 1s in the last bucket is (in the worst case) 50% of the existent 1s in the last k bits, our error is at most 50%.\n",
    "\n",
    "\n",
    "#### Not enough? Add more blocks per power-of-two value\n",
    "\n",
    "We can improve this result by increasing the number of blocks (r) per power-of-two. This means we have, for instance, r=4\n",
    "\n",
    "\n",
    "```\n",
    "    ....16-16-16-16-8-8-8-8-4-4-4-4-2-2-2-2-1-1-1-1\n",
    "``` \n",
    "\n",
    "\n",
    "**In this case, the error is O(1/r)**\n",
    "\n",
    "By picking r appropriately, we can tradeoff **between number of bits we store** and the **error bound**\n",
    "\n",
    "\n",
    "## OTHER USAGES FOR DGIM METHOD\n",
    "\n",
    "Can we handle the case where the stream is not bits, but integers, and we want the sum\n",
    "of the last k elements? Yes.\n",
    "\n",
    "Why should we? **For instance, we want to calculate the Avg. price of last k sales.**\n",
    "\n",
    "### If you know all have at most m bits:\n",
    "\n",
    "- Treat each bit of the m bits of each integer as a separate stream\n",
    "- Use DGIM to count 1s in each integer ($c_i$)\n",
    "- We can calculate the total sum using\n",
    "    \n",
    "$$\\sum_{i=0}^{m-1}c_i2^i$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FLAJOLET-MARTIN METHOD \n",
    "## Counting Distinct Occorrences \n",
    "----"
   ]
  },
  {
   "attachments": {
    "image.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Usage:\n",
    "\n",
    "- How many different Web pages does each customer request in a week?\n",
    "- How many distinct products have we sold in the last week?\n",
    "\n",
    "#### Characteristics\n",
    "\n",
    "\n",
    "- Data stream consists of a universe of elements chosen from a set of size N\n",
    "- Maintain a count of the number of distinct elements seen so far\n",
    "\n",
    "\n",
    "#### Obvious Approach \n",
    "\n",
    "**keep an hash with the set of elements seen so far**\n",
    "\n",
    "#### What if we do not have space to maintain the set of elements seen so far?\n",
    "\n",
    "and we want to...\n",
    "\n",
    "- Estimate the count in an unbiased way\n",
    "- Accept that the count may have a little error, but limit the probability that the error is large\n",
    "\n",
    "\n",
    "### FLAJOLET-MARTIN METHOD \n",
    "\n",
    "- Pick a hash function h that **maps randomly** each of the N elements to at least $log_2N$ bits ( binary representation for instance!) \n",
    "- For each stream element ```a```, let r(a) be the number of trailing 0s in h(a).\n",
    "\n",
    "> say h(a) = 12, then 12 is 1100 in binary, so r(a) = 2\n",
    "    \n",
    "    \n",
    "- Record R = the maximum r(a) seen\n",
    "- In That Case, **the estimated number of distinct elements is equal to $2^R$**\n",
    "\n",
    "(WHAT???)\n",
    "\n",
    "#### How it works empirically:\n",
    "\n",
    "- $h(a)$ is a sequence of $log_2 N$ bits\n",
    "\n",
    "\n",
    "- What fraction of all \"a\" values have a tail of r zeros:\n",
    "> - About 50% of as hash to ***0\n",
    "> - About 25% of as hash to **00\n",
    "> - About 12,5% of as hash to *000\n",
    "\n",
    "\n",
    "That means \n",
    "\n",
    "- $2^{-r}$ fraction of all \"a\" values have a tail of r zeros:\n",
    "\n",
    "**So, if we saw the longest tail of r=2 (i.e., item hash ending *100) then we have probably seen about 4 distinct items so far**\n",
    "\n",
    "This means that, on average, it takes to hash about $2^r$ items before we see one with zero-suffix of length r;\n",
    "\n",
    "\n",
    "#### How it works formally:\n",
    "\n",
    "- What is the probability that a given h(a) ends in at least r zeros is $2^{-r}$?\n",
    "- h(a) hashes elements uniformly at random   \n",
    "- Probability that a random number ends in at least r zeros is $2^{-r}$\n",
    "\n",
    "Then, the probability of NOT seeing a tail of length r among m elements is\n",
    "\n",
    "\n",
    "<img src=\"images/FLAJOLET-MARTIN.png\" style=\"width:40%\"/>\n",
    "\n",
    "Note that \n",
    "\n",
    "![image.png](attachment:image.png)\n",
    "\n",
    "- if m >> $2^r$, then prob. tends to 0\n",
    "- if m << $2^r$, then prob. tends to 1\n",
    "\n",
    "> This means that $2^r$ will almost always be around m\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FINDING SIMILAR ITEMS\n",
    "for instance, finding near-duplicate pages\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Many other problems can be expressed as finding “similar” sets, that is find near-neighbors in high-dimensional space   \n",
    "\n",
    "#### Examples:\n",
    "\n",
    "- Pages with similar words - For duplicate detection, classification by topic\n",
    "- Customers who purchased similar products - Products with similar customer sets\n",
    "- Images with similar features - Users who visited similar websites"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The problem could be stated as: \n",
    "\n",
    "$N$ data points  $x_1, x_2,...$ where each is high dimensional, and $N$ is big.\n",
    "\n",
    "$x_i$ could be for instance, \n",
    "\n",
    "- large documents\n",
    "- images\n",
    "- structured data\n",
    "\n",
    "Goal: \n",
    "\n",
    "- find all pairs of data where $d(x_i,x_j) < s$ where d is a distance measure;\n",
    "- Naïve solution would take $O(N^2)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Jaccard Similarity and Distance of Sets\n",
    "\n",
    "- Jaccard Similarity:\n",
    "\n",
    "$$SIM(C1, C2) = \\frac{|C1 ∩ C2|}{|C1 ∪ C2|}$$\n",
    "\n",
    "- Jaccard Distance:\n",
    "\n",
    "$$D(C1, C2) = 1-\\frac{|C1 ∩ C2|}{|C1 ∪ C2|}$$\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "<img src=\"images/jaccard.png\" style=\"width:60%\"/>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Solution\n",
    "\n",
    "For each element $x_i$ calculate the distance $d(x_i,x_j)$ to every other j elements.\n",
    "\n",
    "This means $\\frac{N(N-1)}{2}$ calculations\n",
    "\n",
    "\n",
    "Suppose we need to apply this to N = 10 million registers\n",
    "\n",
    "\n",
    "If each comparison takes $1\\mu s$ it would take more than a year ....\n",
    "\n",
    "\n",
    "##  Finding Similar Items Efficiently\n",
    "\n",
    "Using only candidate pairs instead all N items we can strongly reduce the the naive approach to a-priori approach. Let us find a priori candidate pairs!\n",
    "\n",
    "\n",
    "\n",
    "```\n",
    "Shingling => Min-Hashing => Locality-Sensitive Hashing\n",
    "```\n",
    "\n",
    "<img src=\"images/lsh.png\" style=\"width:60%\"/>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Shingling \n",
    "- Convert documents to sets\n",
    "\n",
    "Basic principle:\n",
    "\n",
    "A document is a string of characters. Define a k-shingle for a document to be\n",
    "any substring of length k found within the document.\n",
    "\n",
    "> Considering the string D = \"abcdabd\", and we pick k = 2. Then the set of 2-shingles for D is D1 = {ab,bc,cd,da,bd}.\n",
    "\n",
    "> k should be picked large enough that the probability of any given shingle appearing in any given document is low.\n",
    "\n",
    "Thus, if our corpus of documents is emails, picking k = 5 should be fine. To see why, suppose that only letters and a general white-space character appear in emails (randomly). If so, then there would be $27^5 = 14,348,907$ possible shingles. Since the typical email is much smaller than 14 million characters long, we would expect k = 5 to work well, and indeed it does.\n",
    "\n",
    "> but certain letters are use more than others....\n",
    "\n",
    "A good rule of thumb is to imagine that there are only 20 characters (the most used ones) and estimate the number of k-shingles as $20^k$\n",
    "\n",
    "Finally, instead of using substrings directly as shingles for instance \n",
    "\n",
    "$D1 = {ab,bc,cd,da,bd}$\n",
    "\n",
    "we can pick a hash function that maps strings of length k to some number of buckets and treat the resulting bucket number as the shingle:\n",
    "\n",
    "$h(D1) = \\{1, 5, 7,4,2,0 \\}$\n",
    "\n",
    "\n",
    "Each document D can be represented as a binary (0 or 1) vector in the space of k-shingles\n",
    "\n",
    "- Each unique shingle is a dimension\n",
    "- Vectors are very sparse\n",
    "\n",
    "> Documents that have lots of shingles in common have similar text, even if the text appears in different order - we can now measure the common shingles with **Jaccard Distance**.\n",
    "\n",
    "If we use a binary vector with the k-shingles do define each document. we can use bitwise calculations:\n",
    "\n",
    "- set intersection as bitwise AND\n",
    "- set union as bitwise OR\n",
    "\n",
    "\n",
    "C1 = 10111; \n",
    "C2 = 10011\n",
    "\n",
    "- **Size of intersection** = 3; \n",
    "- **size of union** = 4,\n",
    "\n",
    "- **Jaccard similarity (not distance)** = 3/4\n",
    "\n",
    "- **Distance**: d(C1,C2) = 1 – (Jaccard similarity) = 1/4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Min-Hashing \n",
    "\n",
    "- Convert large sets to short signatures, while preserving similarity\n",
    "\n",
    "So, we have a very sparse matrix of this form:\n",
    "\n",
    "<img src=\"images/lsh_2.png\" style=\"width:20%\"/>\n",
    "\n",
    "To solve this problem a naive approach would force us to compute all distances for all values of the matrix. We can try to find a signature whose similarity is equal to column similarity:\n",
    "\n",
    "\n",
    "KEY IDEA:\n",
    "\n",
    "“hash” each column C to a small signature h(C) such that:\n",
    "\n",
    "- (1) h(C) is small enough that the signature fits in RAM\n",
    "- sim(C1, C2) is the same as the “similarity” of signatures h(C1) and h(C2)\n",
    "\n",
    "or at least, find a hash function h(·) such that:\n",
    "\n",
    "- If sim(C1,C2) is high, then with high prob. h(C1) = h(C2)   \n",
    "- If sim(C1,C2) is low, then with high prob. h(C1) ≠ h(C2)\n",
    "\n",
    "> There is a suitable hash function for the Jaccard similarity: It is called Min-Hashing\n",
    "\n",
    "1) Let's consider a random permutation $π$ of rows over the boolean spares matrix of shingles\n",
    "\n",
    "2) Define “hash” function $h_π(C)$ the index of the first (in the permuted order $π$) row in which column C has value 1\n",
    "\n",
    "3) Use several (e.g., 100) independent hash functions (that is, permutations) to create a signature of a column\n",
    "\n",
    "\n",
    "<img src=\"images/minhashing_1.png\" style=\"width:40%\"/>\n",
    "\n",
    "\n",
    "#### Why this is minhasshing is a good indicator for similarity?\n",
    "\n",
    "\n",
    "\n",
    "#### In practice - Permuting rows even once is prohibitive\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Locality-Sensitive Hashing (LSH)\n",
    "\n",
    "- Focus on pairs of signatures likely to be from similar documents\n",
    "\n",
    "Find documents with Jaccard similarity at least s (for some similarity threshold, e.g., s=0.8)\n",
    "\n",
    "\n",
    "KEY IDEIA: Use a function f(x,y) that tells whether x and y is a candidate pair: a pair of elements whose similarity must be evaluated"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
