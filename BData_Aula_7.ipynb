{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aula 7 - BigData Algorithms Parte 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODAY\n",
    "\n",
    "\n",
    "- Finding (Filter) Similar Items\n",
    "- Association Rules / Recomending Systems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FINDING SIMILAR ITEMS\n",
    "for instance, finding near-duplicate pages\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Many other problems can be expressed as finding “similar” sets, that is find near-neighbors in high-dimensional space   \n",
    "\n",
    "#### Examples:\n",
    "\n",
    "- Pages with similar words - For duplicate detection, classification by topic\n",
    "- Customers who purchased similar products - Products with similar customer sets\n",
    "- Images with similar features - Users who visited similar websites"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The problem could be stated as: \n",
    "\n",
    "$N$ data points  $x_1, x_2,...$ where each is high dimensional, and $N$ is big.\n",
    "\n",
    "$x_i$ could be for instance, \n",
    "\n",
    "- large documents\n",
    "- images\n",
    "- structured data\n",
    "\n",
    "Goal: \n",
    "\n",
    "- find all pairs of data where $d(x_i,x_j) < s$ where d is a distance measure;\n",
    "- Naïve solution would take $O(N^2)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Jaccard Similarity and Distance of Sets\n",
    "\n",
    "- Jaccard Similarity:\n",
    "\n",
    "$$SIM(C1, C2) = \\frac{|C1 ∩ C2|}{|C1 ∪ C2|}$$\n",
    "\n",
    "- Jaccard Distance:\n",
    "\n",
    "$$D(C1, C2) = 1-\\frac{|C1 ∩ C2|}{|C1 ∪ C2|}$$\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "<img src=\"images/jaccard.png\" style=\"width:60%\"/>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Solution\n",
    "\n",
    "For each element $x_i$ calculate the distance $d(x_i,x_j)$ to every other j elements.\n",
    "\n",
    "This means $\\frac{N(N-1)}{2}$ calculations\n",
    "\n",
    "\n",
    "Suppose we need to apply this to N = 10 million registers\n",
    "\n",
    "\n",
    "If each comparison takes $1\\mu s$ it would take more than a year ....\n",
    "\n",
    "\n",
    "##  Finding Similar Items Efficiently\n",
    "\n",
    "Using only candidate pairs instead all N items we can strongly reduce the the naive approach to a-priori approach. Let us find a priori candidate pairs!\n",
    "\n",
    "\n",
    "\n",
    "```\n",
    "Shingling => Min-Hashing => Locality-Sensitive Hashing\n",
    "```\n",
    "\n",
    "<img src=\"images/lsh.png\" style=\"width:60%\"/>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Shingling \n",
    "- Convert documents to sets\n",
    "\n",
    "Basic principle:\n",
    "\n",
    "A document is a string of characters. Define a k-shingle for a document to be\n",
    "any substring of length k found within the document.\n",
    "\n",
    "> Considering the string D = \"abcdabd\", and we pick k = 2. Then the set of 2-shingles for D is D1 = {ab,bc,cd,da,bd}.\n",
    "\n",
    "> k should be picked large enough that the probability of any given shingle appearing in any given document is low.\n",
    "\n",
    "Thus, if our corpus of documents is emails, picking k = 5 should be fine. To see why, suppose that only letters and a general white-space character appear in emails (randomly). If so, then there would be $27^5 = 14,348,907$ possible shingles. Since the typical email is much smaller than 14 million characters long, we would expect k = 5 to work well, and indeed it does.\n",
    "\n",
    "> but certain letters are use more than others....\n",
    "\n",
    "A good rule of thumb is to imagine that there are only 20 characters (the most used ones) and estimate the number of k-shingles as $20^k$\n",
    "\n",
    "Finally, instead of using substrings directly as shingles for instance \n",
    "\n",
    "$D1 = {ab,bc,cd,da,bd}$\n",
    "\n",
    "we can pick a hash function that maps strings of length k to some number of buckets and treat the resulting bucket number as the shingle:\n",
    "\n",
    "$h(D1) = \\{1, 5, 7,4,2,0 \\}$\n",
    "\n",
    "\n",
    "Each document D can be represented as a binary (0 or 1) vector in the space of k-shingles\n",
    "\n",
    "- Each unique shingle is a dimension\n",
    "- Vectors are very sparse\n",
    "\n",
    "> Documents that have lots of shingles in common have similar text, even if the text appears in different order - we can now measure the common shingles with **Jaccard Distance**.\n",
    "\n",
    "If we use a binary vector with the k-shingles do define each document. we can use bitwise calculations:\n",
    "\n",
    "- set intersection as bitwise AND\n",
    "- set union as bitwise OR\n",
    "\n",
    "\n",
    "C1 = 10111; \n",
    "C2 = 10011\n",
    "\n",
    "- **Size of intersection** = 3; \n",
    "- **size of union** = 4,\n",
    "\n",
    "- **Jaccard similarity (not distance)** = 3/4\n",
    "\n",
    "- **Distance**: d(C1,C2) = 1 – (Jaccard similarity) = 1/4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Min-Hashing \n",
    "\n",
    "- Convert large sets to short signatures, while preserving similarity\n",
    "\n",
    "So, we have a very sparse matrix of this form:\n",
    "\n",
    "<img src=\"images/lsh_2.png\" style=\"width:30%\"/>\n",
    "\n",
    "To solve this problem a naive approach would force us to compute all distances for all values of the matrix. We can try to find a signature whose similarity is equal to column similarity:\n",
    "\n",
    "\n",
    "KEY IDEA:\n",
    "\n",
    "“hash” each column C to a small signature h(C) such that:\n",
    "\n",
    "- h(C) is small enough that the signature fits in RAM\n",
    "- sim(C1, C2) is the same as the “similarity” of signatures h(C1) and h(C2)\n",
    "\n",
    "or at least, find a hash function h(·) such that:\n",
    "\n",
    "- If sim(C1,C2) is high, then with high prob. h(C1) = h(C2)   \n",
    "- If sim(C1,C2) is low, then with high prob. h(C1) ≠ h(C2)\n",
    "\n",
    "> There is a suitable hash function for the Jaccard similarity: It is called Min-Hashing\n",
    "\n",
    "1) Let's consider a random permutation $π$ of rows over the boolean spares matrix of shingles\n",
    "\n",
    "2) Define “hash” function $h_π(C)$ the index of the first (in the permuted order $π$) row in which column C has value 1\n",
    "\n",
    "3) Use several (e.g., 100) independent hash functions (that is, permutations) to create a signature of a column\n",
    "\n",
    "\n",
    "<img src=\"images/minhashing_1.png\" style=\"width:60%\"/>\n",
    "\n",
    "\n",
    "#### Why this is minhasshing is a good indicator for similarity?\n",
    "\n",
    "Yes! The similarity of two signatures is the fraction of the hash functions in which they agree.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Locality-Sensitive Hashing (LSH)\n",
    "\n",
    "- Focus on pairs of signatures likely to be from similar documents\n",
    "\n",
    "Find documents with Jaccard similarity at least s (for some similarity threshold, e.g., s=0.8)\n",
    "\n",
    "**LSH GENERAL IDEIA:** Use a function f(x,y) that tells whether x and y is a candidate pair: a pair of elements whose similarity must be evaluated\n",
    "\n",
    "**OPTION 1)** Columns x and y of M are a candidate pair if their signatures agree on at least fraction s of their rows\n",
    "\n",
    "\n",
    "<img src=\"images/minhashmatrix.png\" style=\"width:30%\"/>\n",
    "\n",
    "This means $M(i,x)=M(i,y)$ for at least a frac \"s\" of the values of i\n",
    "\n",
    "**LSH MAIN IDEA** Hash columns of signature matrix M several times\n",
    "\n",
    "This will increase perforamnce, once more:\n",
    "\n",
    "<img src=\"images/LSH_1.png\" style=\"width:50%\"/>\n",
    "\n",
    "- Divide matrix M into b bands of r rows   \n",
    "- For each band, hash its portion of each column to a hash table with k buckets   \n",
    "- Make k as large as possible\n",
    "- Candidate column pairs are those that hash to the same bucket for ≥ 1 band\n",
    "- Tune b and r to catch most similar pairs, but few non-similar pairs\n",
    "\n",
    "<img src=\"images/LSH_3.png\" style=\"width:50%\"/>\n",
    "\n",
    "**Pick:**\n",
    "- The number of Min-Hashes (rows of M)\n",
    "- The number of bands b, and\n",
    "- The number of rows r per band \n",
    "\n",
    "to balance false positives/negatives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Association Rules\n",
    "---\n",
    "\n",
    "Given:\n",
    "\n",
    "- A large set of **items**\n",
    "- A large set of **baskets**\n",
    "\n",
    "the main objective is to answer something like this:\n",
    "\n",
    "```\n",
    "People who bought {x,y,z} tend to buy {v,w}\n",
    "\n",
    "```\n",
    "\n",
    "This is an **association rule**.\n",
    "\n",
    "----\n",
    "\n",
    "Consider the following Market-Basket with 5 transactions:\n",
    "\n",
    "<img src=\"images/market_basket.png\" style=\"width:30%\"/>\n",
    "\n",
    "The form of an **association rule** is \n",
    "\n",
    "$$I → j$$\n",
    "\n",
    "where I is a set of items and j is an item. The implication of this association rule is that if all of the items in I appear in some basket, then j is “likely” to appear in that basket as well. - Here, \"likely\" means confidence: let's see some definitions:\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definitions\n",
    "\n",
    "**Support $S(I + j)$:** the ratio of registers that supports our assumption (I and j exists how much times in baskets)\n",
    "\n",
    "$$S(I + j) = \\frac{\\#(I \\cup j)}{T}$$\n",
    "\n",
    "where $T$ =  Total number of Baskets\n",
    "\n",
    "\n",
    "**Confidence $C(I → j)$:** the ratio of baskets with all $I$ and also containing $j$\n",
    "\n",
    "$$C(I → j) = \\frac{\\#(I \\cup j)}{\\#I} = \\frac{S(I + j)}{S(I)}$$\n",
    "\n",
    "\n",
    "In the previous case, S = 0.4 and C = 0.67\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given a support threshold **k**, then sets of items that appear in at least s baskets are called **frequent itemsets**\n",
    "\n",
    "\n",
    "<img src=\"images/market_basket.png\" style=\"width:30%\"/>\n",
    "\n",
    "Example of Rules:\n",
    "\n",
    "- {Milk,Diaper} → {Beer} (S=0.4, C=0.67) \n",
    "- {Milk,Beer} → {Diaper} (S=0.4, C=1.0) \n",
    "- {Diaper,Beer} → {Milk} (S=0.4, C=0.67) \n",
    "- {Beer} → {Milk,Diaper} (S=0.4, C=0.67) \n",
    "- {Diaper} → {Milk,Beer} (S=0.4, C=0.5) \n",
    "- {Milk} → {Diaper,Beer} (S=0.4, C=0.5)\n",
    "\n",
    "Comments:\n",
    "\n",
    "- All the rules above correspond to the same itemset: {Milk, Diaper, Beer}\n",
    "- Rules obtained from the same itemset have identical support but can have different confidence\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to Mine Association Rules?\n",
    "\n",
    "\n",
    "#### brute-force\n",
    "A brute-force approach for mining association rules is to compute the support anw confidence for every possible rule.\n",
    "And take the ones above specific thresholds.\n",
    "\n",
    "This approach is **prohibitively expensive** because there are exponentially many rules that can be extracted from a data set.\n",
    "\n",
    "For N transactions if all had a quantity $d$ of items | we should compare $S$ and $C$ for $M$ itemset candidates per transaction where $M = 2^d-1$ (as we will see) - This means a \n",
    "\n",
    "$$O(NM)$$\n",
    "\n",
    "complexity calculation.\n",
    "\n",
    "<img src=\"images/transactions_1.png\" style=\"width:50%\"/>\n",
    "\n",
    "For instance $N=100$ transactions with 20 items each ($w = 20$) will need $100 \\times 2^{20} = 104 \\space 857 \\space 600$ comparisons (!!) \n",
    "\n",
    "---\n",
    "\n",
    "#### Two step approach:\n",
    "\n",
    "1.  Generate all frequent itemsets (sets of items whose ```Support > K``` )\n",
    "2.  Generate high confidence association rules from each frequent itemset\n",
    "\n",
    "–  Each rule is a partition of a frequent itemset because high support and high confidence just happens for frequent items (High Support is just that!)\n",
    "\n",
    "\n",
    "> Frequent itemset generation is the hardest operation\n",
    "\n",
    "----\n",
    "\n",
    "### Generating Frequent Itemsets ($M$):\n",
    "\n",
    "If we have $d$ items in a transaction, the number of possible non empty frequent transactions is given by \n",
    "\n",
    "$$\\sum_{i=0}^N \\binom{w}{i} = 2^w - 1$$\n",
    "\n",
    "This quest needs something that reduces our search process. What if we can filter them using some kind of previously known info? \n",
    "\n",
    "> Let's imagine that a pair $w$ is not very frequent. what happens to any itemset composed by $w$?\n",
    "\n",
    "<img src=\"images/apriori.png\" style=\"width:50%\"/>\n",
    "\n",
    "> they are not very frequent also! \n",
    "\n",
    "We can state:\n",
    "> If a set of items I appears at least s times, so does every subset J of I\n",
    "\n",
    "So we can reduce our search to pairs:\n",
    "\n",
    "\n",
    "1. Read baskets and count in main memory the occurrences of each individual item \n",
    "    - Needs #$items$ hash\n",
    "2. Read baskets again and count in main memory only those pairs where both elements are frequent (from Pass 1) and filter them.\n",
    "    - Needs list of frequent items + #$items^2$ hash \n",
    "3. And then move on the the next 3-itemsets\n",
    "\n",
    "#### Let's see an example of apriori usage:\n",
    "\n",
    "CONSIDER: \n",
    "\n",
    "- Ck = candidate k-tuples = those that might be frequent sets (support > s) based on information from the pass for k–1\n",
    "- Lk = the set of truly frequent k-tuples\n",
    "\n",
    "\n",
    "EXAMPLE: \n",
    "\n",
    "- C1 ={{b}{c}{j}{m}{n}{p}}\n",
    "- Count the support of itemsets in C1\n",
    "- Prune non-frequent: L1 = { b, c, j, m }\n",
    "- Generate C2 = { {b,c} {b,j} {b,m} {c,j} {c,m} {j,m} }\n",
    "- Count the support of itemsets in C2\n",
    "- Prune non-frequent: L2 = { {b,m} {b,c} {c,m} {c,j} }\n",
    "- Generate C3 = { {b,c,m} {b,c,j} {b,m,j} {c,m,j} }\n",
    "- Count the support of itemsets in C3\n",
    "- Prune non-frequent: L3 = { {b,c,m} }\n",
    "\n",
    "\n",
    "<img src=\"images/apriori_2.png\" style=\"width:60%\"/>\n",
    "\n",
    "\n",
    "----\n",
    "\n",
    "### Generate high confidence association rules from each frequent itemset\n",
    "\n",
    "Find all non-empty subsets f ⊂ L such that f → L – f satisfies the minimum confidence requirement.\n",
    "(Support is already assured for being a \"frequent itemset\")\n",
    "\n",
    "\n",
    "–  Dado {A,B,C,D} como um frequent itemset, estas são as \"rules\" candidatas:\n",
    "\n",
    "- ABC → D | A → BCD | AB → CD | BD → AC\n",
    "- ABD → C | B → ACD | AC →  BD | CD → AB\n",
    "- ACD → B | C → ABD | AD →  BC\n",
    "- BCD → A | D → ABC | BC → AD\n",
    "\n",
    "Como calcular a confiança (C) dados os suportes:\n",
    "\n",
    "$$C(A,B → C,D) = \\frac{S(A,B,C,D)}{S(A,B)} = \\frac{\\#(A,B,C,D)}{\\#(A,B)}$$\n",
    "\n",
    "Sendo a última expressão a definifção de C(A,B → C,D)\n",
    "\n",
    "\n",
    "\n",
    "## PCY (Park, Chen, Yu)\n",
    "\n",
    "In pass 1 of A-Priori, most memory is idle (not used)\n",
    "\n",
    "- We store only individual item counts\n",
    "\n",
    "FIRST IDEA:\n",
    "\n",
    "In addition to item counts, maintain a hash table with as many buckets as fit in memory: In each Bucket we keep the count for each bucket into which pairs of items are hashed;\n",
    "\n",
    "So, in Pass 1 of PCY we:\n",
    "\n",
    "#### PCY PASS 1\n",
    "- We store individual item counts;\n",
    "- Store a count of hashed pairs of items for each bucket;\n",
    "\n",
    "\n",
    "> OBSERVATIONS: \n",
    "> - If a bucket contains a frequent pair, then the bucket is surely frequent\n",
    "> - However, even without any frequent pair, a bucket can still be frequent\n",
    "\n",
    "So, for a bucket with total count less than s, none of its pairs can be frequent and so, **Pairs that hash to this bucket can be eliminated as candidates** even if they are frequent.\n",
    "\n",
    "#### PCY PASS 2\n",
    "\n",
    "Only count pairs that hash to frequent buckets\n",
    "\n",
    "\n",
    "SECOND IDEA:\n",
    "\n",
    "Implementation can use a \"kind of\" Bloom Filter:\n",
    "\n",
    "> We keep a bit vector instead of buckets: 1 means the bucket \"is frequent\", that is, it has elements that added are bigger than s\n",
    "\n",
    "<img src=\"images/PCY.png\" style=\"width:50%\" />\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
