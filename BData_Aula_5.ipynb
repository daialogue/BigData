{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Big Data - Aula 5\n",
    "\n",
    "---\n",
    "\n",
    "## Big Data Technological Concepts\n",
    "## Theorectical Approaches\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Map-Reduce Concept\n",
    "---\n",
    "From all previous examples, we conclude that ***the challenge number 1*** is:\n",
    "\n",
    "\n",
    "### How to distribute computation, with the following characteristics:\n",
    "\n",
    "- Commodity Linux nodes\n",
    "- Commodity network (ethernet) to connect them\n",
    "- Without losses due to strong resilient redundancy of nodes (machines) during calculations (no losses over failover)\n",
    "- DataCenter Driven \n",
    "\n",
    "Tipically, distributed (scientific) computation is done with supercomputers - which is not a datacenter -with special-purpose parallel computers and specialized hardware;\n",
    "\n",
    "<img src=\"images/datacenter.png\" style=\"width:60%\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### New Ideas:\n",
    "\n",
    "- distributed file system - Huge partion number with replication of data or redundancy to protect against the frequent media failures\n",
    "\n",
    "- Organized in virtual servers or dockers /physical servers/racks/switches\n",
    "\n",
    "<img src=\"images/racks.png\" style=\"width:60%\"/>\n",
    "\n",
    "\n",
    "- Recognize that It is a fact of life that components fail, and the more components, such as compute nodes and communication links, a system has, the more frequently something in the system will not be working at any given time;\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Is lack of Redundacy a Big Problem?\n",
    "\n",
    "- One server may stay up 3 years (~ 1,000 days)\n",
    "- If you have 1,000 servers, expect to loose 1/day\n",
    "- People estimated Google had ~1M machines in 2011 (!!) so, \n",
    "\n",
    "***1,000 machines fail every day and One single BigData calculation takes hours on thousands of compute nodes***\n",
    "\n",
    "***BigData calculation is restarted again and again...***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This is solved either by:\n",
    "\n",
    "- Files must be stored redundantly. If we did not duplicate the file at several compute nodes, then if one node failed, all its files would be unavailable until the node is replaced - **DISTRIBUTED FILE SYSTEM**\n",
    "\n",
    "\n",
    "- Computations must be divided into tasks, such that if any one task fails to execute to completion, it can be restarted without affecting other tasks. This strategy is followed by the MapReduce programming - **MAPREDUCE PROGRAMMING MODEL**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Distributed File Systems: \n",
    "\n",
    "---\n",
    "\n",
    "> Hadoop: HDFS; \n",
    "> Google: GFS;\n",
    "\n",
    "\n",
    "<img src=\"images/hadoop-logo.png\" style=\"width:50%\"/>\n",
    "\n",
    "\n",
    "Composed by:\n",
    "\n",
    "## Chunk servers\n",
    "\n",
    "- File is split into contiguous chunks\n",
    "- Typically each chunk is 16-64MB\n",
    "- Each chunk replicated (usually 2x or 3x)\n",
    "- Try to keep replicas in different racks\n",
    "\n",
    "\n",
    "## Master node\n",
    "\n",
    "- a.k.a. Name Node in Hadoop’s HDFS\n",
    "- Stores metadata about where files are stored\n",
    "- Might be replicated\n",
    "\n",
    "## Client library for file access\n",
    "\n",
    "- Talks to master to find chunk servers\n",
    "- Connects directly to chunk servers to access data\n",
    "\n",
    "<img src=\"images/chunks.png\" style=\"width:60%\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MapReduce Design Pattern\n",
    "\n",
    "---\n",
    "#### Was created by google to execute very large matrix-vector multiplications to be used initially in the calculation of PageRank\n",
    "\n",
    "\n",
    "Computing Style that automatically manage large-scale, parallel computations in a way that is tolerant of hardware faults.\n",
    "\n",
    "You Only need to define 2 functions: Map and Reduce. \n",
    "\n",
    "<img src=\"images/mapreduce_basics.png\" style=\"width:50%\"/>\n",
    "\n",
    "1. **Split** records in chunks\n",
    "1. **MAP:** - Use nodes to extract something of interest from each using the same **Map** function\n",
    "1. **GROUP BY KEY** - Aggregate by `key` intermediate results \n",
    "1. **REDUCE** - Generate final output using a **Reducer** Function, that returns a single result for each `key`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple Intuition\n",
    "\n",
    "Let's consider a standard example application: **counting the number of occurrences in for each word in a collection of documents.**\n",
    "\n",
    "<img src=\"images/MR_0.png\" style=\"width:70%\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) SPLIT IN CHUNKS:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys([0, 1, 2, 3, 4, 5])"
      ]
     },
     "execution_count": 305,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunks = {0:\"O incêndio florestal de Pedrógão Grande deflagrou a 17 de junho de 2017 no concelho de Pedrógão Grande, no distrito de Leiria, em Portugal, tendo alastrado aos concelhos vizinhos de Castanheira de Pêra, Figueiró dos Vinhos, Ansião e Alvaiázere (também distrito de Leiria); ao concelho da Sertã (distrito de Castelo Branco); ao concelho de Pampilhosa da Serra (distrito de Coimbra).\",1:\"No mesmo dia deflagrou outro incêndio de grandes proporções no concelho de Góis, distrito de Coimbra, que acabou posteriormente por alastrar aos concelhos de Pampilhosa da Serra e de Arganil. No dia 20 de Junho de 2017 uma das frentes de fogo do incêndio de Pedrógão Grande juntou-se ao incêndio de Góis, formando uma área ardida contígua.\",2:\"O desastre é o maior incêndio florestal de sempre em Portugal, o mais mortífero da história do país e o 11.º mais mortífero a nível mundial desde 1900. Um balanço provisório contabilizou 64 mortos (63 civis e 1 bombeiro voluntário de Castanheira de Pera) e 254 feridos (241 civis, 12 bombeiros e 1 militar da Guarda Nacional Republicana), dos quais 7 em estado grave (4 bombeiros, 2 civis e 1 criança). Entre as vítimas mortais, 47 foram encontradas nas estradas do concelho de Pedrógão Grande, tendo 30 morrido nos automóveis e 17 nas suas imediações durante a fuga ao incêndio. O incêndio também arrasou dezenas de lugares.\",3:\"O nº oficial é de 64 mortos. Em termos de prejuízos materiais, foram contabilizadas mais de 500 casas de habitação parcial ou totalmente destruídas pelo fogo. Foram também afectadas 48 empresas com 372 postos de trabalho. A estimativa provisória do montante total de prejuízos ascende a 500 milhões de euros.\", 4:\"A causa apontada pelas autoridades foi trovoada seca que, conjugada com temperaturas muito elevadas (superiores a 40 graus Celsius) e vento muito intenso e variável, fez deflagrar e propagar rapidamente o fogo. No entanto, o presidente da Liga dos Bombeiros, Jaime Marta Soares, acredita que este incêndio não teve origem em causas naturais já que, segundo a perceção de alguns habitantes de Pedrógão Grande, o fogo já estaria ativo duas horas antes da altura em que ocorreu a trovoada seca nesta zona. A Procuradoria-Geral da República confirmou que o Ministério Público está a investigar as causas do incêndio. Como resposta à catástrofe, o governo de Portugal decretou três dias de luto nacional, de 18 a 20 de junho de 2017, enquanto várias autoridades internacionais enviaram mensagens de solidariedade.\", 5:\"O diabo de fogo é um remoinho que acontece quando no solo há zonas que aquecem mais que outras devido a uma maior presença das chamas. O ar que fica sobre essa zonas fica igualmente mais quente, mais leve, e tende a subir mais rapidamente que o ar em redor. Se o ar que circunda nas imediações tiver tendência para rodar segundo um eixo horizontal e for apanhado por correntes verticais ascendentes, de baixo para cima, o eixo de rotação converte-se numa rotação vertical e a chama acompanha o movimento do vento, fazendo o efeito que as imagens mostram.\"}\n",
    "chunks.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'O nº oficial é de 64 mortos. Em termos de prejuízos materiais, foram contabilizadas mais de 500 casas de habitação parcial ou totalmente destruídas pelo fogo. Foram também afectadas 48 empresas com 372 postos de trabalho. A estimativa provisória do montante total de prejuízos ascende a 500 milhões de euros.'"
      ]
     },
     "execution_count": 309,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunks[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total number of chunks:  6\n",
      "------------------------------\n",
      "total number of map tasks:  3\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 313,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#MASTER hash function to distribute files for m mapping tasks\n",
    "\n",
    "m=3\n",
    "print(\"total number of chunks: \", len(chunks))\n",
    "print(\"------------------------------\")\n",
    "print(\"total number of map tasks: \", m)\n",
    "\n",
    "data_arr = []\n",
    "file_chunks = [{} for i in range(m)]\n",
    "for j in range(len(chunks)):\n",
    "    file_chunks[j%m][j] = chunks[j] \n",
    "\n",
    "len(file_chunks[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) MAP: \n",
    "\n",
    "In this case the Map function is such that: \n",
    "\n",
    "```Map(docChunk) = [{w_1,1}, {w_2,1},...,{w_n,1}]```\n",
    "\n",
    "\n",
    "where ```w_i``` is one of the ```n``` available words in that specific ```docChunk``` (chunk of information from documents). The map function breaks the documents in words and returns `<key:value>` pairs of words found (where the value is allways 1)\n",
    "\n",
    "> Note that if the word \"and\" appears 10 times along the same document the map function will return 10 key-value pairs `<\"and\",1`>\n",
    "\n",
    "- Alternative MAP Function: for each iteration it returns, per docChunk, pairs of type `<\"and\",m`>. This is not a \"pure\" Map function but it makes sense in order to optimize processing. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In-mapper combining\n",
    "\n",
    "> Note that each ```data_arr[i][\"a\"]``` for any ```i``` should return in theory only value 1, because no aggregation should be done in the map server.\n",
    "\n",
    "> However, a previous sum in the map server **can (should) be done everytime the reduce step is associative and commutative**, which allows to save time and network occupation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Map example with in-Mapper combining\n",
    "\n",
    "def my_map(my_file):\n",
    "    vals = \" \".join(my_file.values())\n",
    "    vals =  vals.replace(\",\", \"\").replace(\".\", \"\").replace(\"(\", \"\").replace(\")\", \"\").replace(\";\", \"\").replace(\"º\",\"\")\n",
    "    vals = vals.lower()\n",
    "    vals = vals.split(\" \")\n",
    "    vals = [val for val in vals if not val.isdigit()]\n",
    "\n",
    "\n",
    "    my_dict = {}\n",
    "    for m in vals:\n",
    "        if m in my_dict.keys():\n",
    "            my_dict[m] += 1\n",
    "        else:\n",
    "            my_dict[m] = 1\n",
    "\n",
    "    return my_dict    \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data from map functions:\n",
      "[('o', 2), ('incêndio', 1), ('florestal', 1), ('de', 18), ('pedrógão', 2), ('grande', 2), ('deflagrou', 1)]\n",
      "[('no', 4), ('mesmo', 1), ('dia', 2), ('deflagrou', 1), ('outro', 1), ('incêndio', 5), ('de', 18)]\n"
     ]
    }
   ],
   "source": [
    "#MASTER: LAUNCHING MY MAP FUNCTION in Main \n",
    "\n",
    "# What is the problem with this strategy? compared with a real mapper?\n",
    "# Map Functions should be launched in Parallel (for instance, async calls to my_map)\n",
    "\n",
    "for file in file_chunks: \n",
    "    data_arr.append(my_map(file))\n",
    "\n",
    "print(\"data from map functions:\")    \n",
    "print((list(data_arr[0].items()))[:7])\n",
    "print((list(data_arr[1].items()))[:7])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) GROUPING BY KEY\n",
    "\n",
    "- Probably the most complex aspect of MapReduce execution\n",
    "\n",
    "MAP SIDE:\n",
    "\n",
    "- Map outputs are buffered in memory in a circular buffer\n",
    "- When buffer reaches threshold contents are “spilled” to disk\n",
    "\n",
    "REDUCE SIDE:\n",
    "\n",
    "- First, map outputs are copied over to reducer machine\n",
    "- Final merge pass goes directly into reducer\n",
    "\n",
    "\n",
    "<img src=\"images/shufle_and_sort.png\" style=\"width:60%\"/>\n",
    "\n",
    "\n",
    "### Detailled description:\n",
    "\n",
    "- The master controller process knows how many Reduce tasks there will be, say r such tasks (The user typically tells the MapReduce system what r should be.) \n",
    "\n",
    "- Then the master controller picks a hash function that takes a key as argument and produces a bucket number from 0 to r − 1. \n",
    "\n",
    "- Each ```key``` that is output by a Map task is hashed and its key-value pair is put in one of r **local files**.\n",
    "\n",
    "- To perform the grouping by key and distribution to the Reduce tasks, the master controller **merges the files from each Map task that are destined for a particular Reduce task** and feeds the merged file to that process as a sequence of key/list-of-values pairs.\n",
    "\n",
    "- For each key k, the input of the Reduce task that handles key k will receive a pair of the form \n",
    "\n",
    "    ```(k, [v1, v2, . . . , vn])```\n",
    "    \n",
    "    where ```(k,v1), (k,v2), ..., (k,vn)```\n",
    "    \n",
    "    are all the key-value pairs with key k coming from all the Map tasks. Note that this split task is actually done using a hash function for R Reduce servers over the key to define which server should process it.\n",
    "\n",
    "\n",
    "Here it is a Master process for this GROUPING BY KEY:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of map tasks results 3\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[18, 18, 8]"
      ]
     },
     "execution_count": 321,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#MASTER: GroupbyKeys, and Send files to Reducers.\n",
    "\n",
    "#number of reducers\n",
    "R = 2\n",
    "\n",
    "#number of map tasks results\n",
    "print(\"number of map tasks results\", len(data_arr))\n",
    "\n",
    "reducer_data = [{} for i in range(R)]\n",
    "\n",
    "# for each Map output file\n",
    "for d in data_arr:\n",
    "    \n",
    "    # for each key (j) in each file (d) from a map function, \"aggregate keys\"\n",
    "    for j,j_val in d.items():\n",
    "        if j not in reducer_data[sum(map(ord,j))%R].keys():\n",
    "            reducer_data[sum(map(ord,j))%R][j] = [j_val]\n",
    "        else:\n",
    "            reducer_data[sum(map(ord,j))%R][j].append(j_val)\n",
    "        \n",
    "\n",
    "# Data for the first reducer \n",
    "reducer_data[0].keys()\n",
    "\n",
    "reducer_data[1][\"de\"]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4) Reduce\n",
    "\n",
    "The Reduce Step agregates data to summarize the final results.\n",
    "\n",
    "We receive, several key-value groups and the result is then a final key value pair;\n",
    "\n",
    "\n",
    "In this case, the grouping is just a sum across all data from the mappings.\n",
    "\n",
    "\n",
    "### skew \n",
    "\n",
    "There could be a significant difference in the amount of time each reducer takes (skew). We can reduce the impact of skew by using fewer Reduce tasks than there are reducers. If keys are sent randomly to Reduce tasks, we can expect that there will be some averaging of the total time required by the different Reduce tasks.\n",
    "\n",
    "----\n",
    "\n",
    "In this case we should have something like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def my_reducer(my_items):\n",
    "    res = {}\n",
    "    #para cada item, soma os seus valores\n",
    "    for i,j in my_items.items():\n",
    "        res[i] = sum(j)\n",
    "\n",
    "    return res\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final data: {'o': 19, 'de': 44, 'pedrógão': 5, 'grande': 5, 'deflagrou': 2, 'a': 15, 'no': 7, 'concelho': 5, 'alastrado': 1, 'aos': 2, 'castanheira': 2, 'pêra': 1, 'vinhos': 1, 'ansião': 1, 'e': 14, 'da': 8, 'sertã': 1, 'castelo': 1, 'branco': 1, 'serra': 2, 'coimbra': 2, 'oficial': 1, 'é': 3, 'prejuízos': 2, 'materiais': 1, 'foram': 3, 'casas': 1, 'totalmente': 1, 'fogo': 5, 'com': 2, 'trabalho': 1, 'estimativa': 1, 'do': 6, 'ascende': 1, 'milhões': 1, 'mesmo': 1, 'outro': 1, 'que': 13, 'acabou': 1, 'por': 2, 'uma': 3, 'frentes': 1, 'área': 1, 'ardida': 1, 'causa': 1, 'pelas': 1, 'autoridades': 2, 'temperaturas': 1, 'elevadas': 1, 'superiores': 1, 'fez': 1, 'entanto': 1, 'presidente': 1, 'liga': 1, 'marta': 1, 'soares': 1, 'acredita': 1, 'este': 1, 'origem': 1, 'naturais': 1, 'já': 2, 'segundo': 2, 'habitantes': 1, 'estaria': 1, 'ativo': 1, 'duas': 1, 'horas': 1, 'antes': 1, 'altura': 1, 'ocorreu': 1, 'nesta': 1, 'procuradoria-geral': 1, 'ministério': 1, 'público': 1, 'está': 1, 'resposta': 1, 'decretou': 1, 'três': 1, 'dias': 1, 'nacional': 2, 'enquanto': 1, 'internacionais': 1, 'enviaram': 1, 'mensagens': 1, 'desastre': 1, 'mortífero': 2, 'história': 1, 'país': 1, 'desde': 1, 'contabilizou': 1, 'bombeiro': 1, 'voluntário': 1, 'quais': 1, 'grave': 1, 'criança': 1, 'vítimas': 1, 'mortais': 1, 'estradas': 1, 'imediações': 2, 'durante': 1, 'fuga': 1, 'arrasou': 1, 'lugares': 1, 'diabo': 1, 'remoinho': 1, 'solo': 1, 'há': 1, 'zonas': 2, 'aquecem': 1, 'devido': 1, 'presença': 1, 'chamas': 1, 'ar': 3, 'fica': 2, 'sobre': 1, 'igualmente': 1, 'subir': 1, 'circunda': 1, 'eixo': 2, 'for': 1, 'correntes': 1, 'ascendentes': 1, 'baixo': 1, 'rotação': 2, 'converte-se': 1, 'numa': 1, 'fazendo': 1, 'mostram': 1}\n"
     ]
    }
   ],
   "source": [
    "#MASTER: Call Reducers with their data.\n",
    "#same async problem...\n",
    "\n",
    "for red in reducer_data:\n",
    "    res = my_reducer(red)\n",
    "\n",
    "print(\"final data:\", res)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summarizing MapReduce Steps\n",
    "---\n",
    "\n",
    "- Sequentially **READ** a lot of data; \n",
    "   \n",
    "- **MAP**: Extract something you care about, per register (it's about you)   \n",
    "    - ```Map(<k, v>) → <k’, v’>```\n",
    "    - There is one Map call for every ```<k,v>``` pair\n",
    "    \n",
    "   \n",
    "- **GROUP BY KEY**: Sort and Shuffle  (Always the same operation, **sort by key**)  \n",
    "   \n",
    "- **REDUCE**: Aggregate, summarize, filter or transform    \n",
    "    - ``` Reduce(<k’, [v’,...]>) → <k’, v’’>```\n",
    "    - All values v’ with same key k’ are reduced together and processed in v’ order\n",
    "    - There is one Reduce func6on call per unique key k’\n",
    "    \n",
    "    \n",
    "- **WRITE** the result\n",
    "\n",
    "---\n",
    "\n",
    "<img src=\"images/MR_3.png\" style=\"width:90%\" />\n",
    "\n",
    "Let's See later we can do almost anything with this strategy, but it is loosing hype in this moment for a more generalized approach."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Map Reduce Takes care of:\n",
    "\n",
    "- Partitioning the input data\n",
    "- Scheduling the program’s execution across a set of machines\n",
    "- Performing the group by key step    \n",
    "- Handling machine failures\n",
    "- Managing required inter-machine communication\n",
    "\n",
    "\n",
    "## But...\n",
    "\n",
    "- Gives you Limited control over data and execution flow    \n",
    "- All algorithms must be expressed in map, reduced (GROUP BY KEY Procedure - also known as **shuffle and sort**)\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "## Considering there is M map tasks, R reduce tasks:\n",
    "\n",
    "- M is normally much larger than the number of nodes in the cluster (```map tasks >> machines```);\n",
    "    - One DFS (Distributed File System) chunk per map is common\n",
    "    - Improves dynamic load balancing and speeds up recovery from worker failures\n",
    "\n",
    "- Usually R is smaller than M\n",
    "    - Because output is spread across R files\n",
    "    \n",
    "## Typical Problems \n",
    "\n",
    "- Slow workers significantly lengthen the job completion time\n",
    "\n",
    "> Solution: Near end phase, make copies of the same tasks: whichever one finishes first “wins”\n",
    "\n",
    "## Good Advises\n",
    "\n",
    "#### Debugging\n",
    "\n",
    "- Independently tested (e.g.,unittesting)\n",
    "- Compose (tested) components in mappers and reducers\n",
    "\n",
    "\n",
    "#### Avoid...\n",
    "\n",
    "- Avoid object creation\n",
    "    - (Relatively)costly operation\n",
    "    - Garbage collection\n",
    "\n",
    "\n",
    "- Avoid buffering\n",
    "    - Limited heap size\n",
    "    - Works for small datasets,but won’t scale!\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Usage Examples\n",
    "---\n",
    "\n",
    "#### Very important: The entire distributed-file-system makes sense only when files are very large and are rarely updated in place. Updates makes DFS very slow!\n",
    "\n",
    "- Simple Algorithms (Count Distinct (explained), Computing the Mean)\n",
    "- Matrix-Matrix Multiplication\n",
    "- Matrix-Vector Multiplication\n",
    "- Relational Algebra operations (SQL or similar)\n",
    "\n",
    "\n",
    "## Count Distinct\n",
    "\n",
    "\n",
    "\n",
    "<img src=\"images/word_count_map.png\" style=\"width:30%\"/>\n",
    "\n",
    "---\n",
    "\n",
    "<div style=\"text-align:center\">....with the \"Group By Key\" magic.....</div>\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "<img src=\"images/word_count_reduce.png\" style=\"width:40%\"/>\n",
    "\n",
    "\n",
    "## Computing the Mean\n",
    "\n",
    "\n",
    "<img src=\"images/mean_map.png\" style=\"width:30%\"/>\n",
    "\n",
    "---\n",
    "\n",
    "<div style=\"text-align:center\">....with the \"Group By Key\" magic.....</div>\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "<img src=\"images/mean_reduce.png\" style=\"width:40%\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matrix Multiplication\n",
    "\n",
    "Web Page Rannking uses matrices with sizes\n",
    "$$M(n\\times n)$$ where $n=10^{12}$ with 10 to 15 nonzero elements per row (sparse). \n",
    "With this n, it has $10^{24}$ elements which means, if dense, it would not be possible to save since $10^{24}$ = 10 Tera of Teras Bytes at least (2 byte per cell).\n",
    "\n",
    "So, when to use Distributed FS and/or MapReduce for this calculation? ...\n",
    "\n",
    "\n",
    "### If the Vector v Cannot Fit in Main Memory\n",
    "\n",
    "The Vector $\\textbf{x} = \\textbf{Mv}$\n",
    "$$x_i = \\sum_i^N m_{ij}v_j$$\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "If the problem is huge (no row fits in main memory) we can divide the matrix in strypes:\n",
    "\n",
    "<img src=\"images/matrix_mul.png\" style=\"width:40%\"/>\n",
    "\n",
    "#### MAP\n",
    "\n",
    "- Should load at innitialization the vector $\\textbf{v}$ or its partial $\\textbf{v}_m$ (since it will be used accross several tasks) where m is the MAP task;\n",
    "\n",
    "- Only then, each Map task receives as input a stripe $m_s$, for $s={1,...,S}$;\n",
    "\n",
    "- For each task we should receive a mapping to a tuple like $(i, m_{ij}v_j)$,  **where the $i$ is the indexing used to redirect to the right reducer**.\n",
    "\n",
    "\n",
    "#### REDUCER\n",
    "\n",
    "- The Reduce function simply sums all the values associated with a given key $i$. The result will be a pair $(i,x_i)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Queries With MapReduce\n",
    "\n",
    "-----\n",
    "\n",
    "\n",
    "### Computing Selection (where clause in SQL)  or Projections (choosing specific fields in SQL removing duplicates)\n",
    "\n",
    "Outputs only those tuples that satisfy C in a set of R tuples (Registers in SQL)\n",
    "\n",
    "#### Map\n",
    "\n",
    "For each tuple t in R, test if it satisfies C. If so, produce the key-value pair ```(t,t)```. That is, both the key and value are t. If t does not satisfy C, then the mapper for t produces nothing.\n",
    "\n",
    "If a projection is asked, output the key-value pair ```(t′, t′)```.\n",
    "\n",
    "\n",
    "#### Reduce\n",
    "\n",
    "The Reduce function is the identity. It simply passes each key-value pair to the output. \n",
    "\n",
    "If a projection is asked, the reducer will receive something like ```(t′, [t′, t′, t′, t′....])```. The Reduce function turns ```(t′, [t′, t′, . . . , t′])``` into ```(t′, t′)```, so it produces exactly one pair ```(t′,t′)``` for this key ```t′```.\n",
    "\n",
    "-----\n",
    "\n",
    "\n",
    "### Computing Natural Join by MapReduce (Inner Join clause in SQL) \n",
    "\n",
    "From a list with fields ```(a,b)``` and ```(b,c)``` outputs ```(a,b,c)``` where ```b``` are equal\n",
    "\n",
    "#### Map\n",
    "\n",
    "For each tuple ```(a,b)``` of R, produce the key-value pair  ```(b, (R, a))``` . For each tuple ```(b, c)``` of S, produce the key-value pair  ```(b, (S, c))```.\n",
    "\n",
    "#### Reduce\n",
    "\n",
    "Each key value ```b``` will be associated with a list of pairs that are either of the form ```(R, a)``` or ```(S, c)```. The Reduce will construct all pairs consisting of one with first component R and the other with first component S, say ```(R,a)``` and ```(S, c)```. \n",
    "\n",
    "- The output from this key and value list is a sequence of key-value pairs. \n",
    "- The key is irrelevant. \n",
    "- Each value is one of the triples (a, b, c) such that (R, a) and (S, c) are on the input list of values for key b.\n",
    "\n",
    "\n",
    "----\n",
    "\n",
    "### Computing Grouping and Aggregation by MapReduce\n",
    "\n",
    "Let ```R(a,b,c)``` be a relation to which we apply the an Grouping and Aggregation operator. Map will perform the grouping, while Reduce does the aggregation.\n",
    "Imagine there is one grouping attribute ```(a)```, one aggregated attribute ```(b)```, and one attribute ```(c)``` that is neither grouped nor aggregated.\n",
    "\n",
    "#### Map\n",
    "\n",
    "For each tuple ```(a, b, c)``` produce the key-value pair ```(a, b)``` \n",
    "\n",
    "\n",
    "#### Reduce\n",
    "\n",
    "Each key a represents a group. Apply the aggregation operator to the list ```[b1, b2, . . . , bn]``` of B-values associated with key a. The output is the pair ```(a,x)```, where x is the result of applying the aggregaotr operator to the list. \n",
    "\n",
    "For example, if this operator is ```SUM```, then ```x = b1 +b2 +···+bn```, and if it is ```MAX```, then ```x``` is the largest of ```b1, b2,...,bn```, etc.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Workflow Systems (Spark)\n",
    "---\n",
    "\n",
    "<img src=\"images/spark_logo.jpeg\" style=\"width:20%\"/>\n",
    "\n",
    "### THE Answer to “What’s beyond MapReduce?”\n",
    "### \"Where the Hype is\"\n",
    "### Commercial Support Product (DATABRICKS), Appache open source project \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "----\n",
    "\n",
    "\n",
    "\n",
    "Workflow systems extend MapReduce from the simple two-step workflow (the Map function feeds the Reduce function) \n",
    "\n",
    "to \n",
    "\n",
    "\n",
    "any collection of functions, with an acyclic graph representing workflow among the functions. That is, there is an acyclic flow graph whose arcs a → b represent the fact that function a’s output is an input to function b.\n",
    "\n",
    "EXAMPLE:\n",
    "\n",
    "\n",
    "<img src=\"images/workflow_1.png\" style=\"width:50%\"/>\n",
    "\n",
    "\n",
    "\n",
    "- The data passed from one function to the next is a file of elements of one type. \n",
    "\n",
    "- If a function has a single input, then that function is applied to each input independently, just as Map and Reduce functions are applied to their input elements individually.\n",
    "\n",
    "- If a function has several inputs from more than one file, elements from each of the files can be combined in various ways. But the function itself is applied to combinations of input elements, at most one from each input file.\n",
    "\n",
    "\n",
    "### MapReduce Analogy:\n",
    "\n",
    "- There can be many tasks per function of a workflow\n",
    "- A master controller is responsible for dividing the work among the tasks\n",
    "- The **blocking property** is also valid but more relevant now: workflow functions only deliver output after they complete.\n",
    "- Some applications of workflow systems are effectively cascades of MapReduce jobs. An example would be the join of three relations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark\n",
    "\n",
    "Spark is, at its heart, a workflow system. However, it is an advance over the early workflow systems in several ways, including:\n",
    "\n",
    "- An efficient way of coping with failures.\n",
    "- Integration of programming language features such as looping (there goes our acyclic graph represention...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RDDs , DataFrames and other  Concept\n",
    "---\n",
    "\n",
    "Spark has several core abstractions: Datasets, DataFrames, SQL Tables, and Resilient Distributed Datasets\n",
    "(RDDs). These abstractions all represent distributed collections of data.\n",
    "\n",
    "\n",
    "> The DataFrame concept is not unique to Spark. R and Python both have similar concepts. However, Python/R DataFrames (with some exceptions) exist on one machine rather than multiple machines. This limits what you can do with a given DataFrame in python and R to the resources that exist on that specific machine. However, since Spark has language interfaces for both Python and R, it’s quite easy to convert to Pandas (Python) DataFrames to Spark DataFrames and R DataFrames to Spark DataFrames (in R).\n",
    "\n",
    "\n",
    "#### Central data abstraction of Spark: **RDD - Resilient Distributed Dataset** \n",
    "\n",
    "\n",
    "<img src=\"images/spark_data_stuct.png\" style=\"width:50%\"/>\n",
    "\n",
    "> DataFrames and Datasets are built on top of RDDs.\n",
    "\n",
    "#### A DataFrame is now the most common Structured API for Spark, and simply represents a table of data with rows and columns. The list of columns and the types in those columns the schema.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "<img src=\"images/dataframes.png\" style=\"width:50%\"/>\n",
    "\n",
    "\n",
    "#### Transformations\n",
    "In Spark, the core data structures are immutable meaning they cannot be changed once created. \n",
    "\n",
    "In order to “change” a DataFrame you will have to call a Transformation.  There exists two kinds of Transformations:\n",
    "\n",
    "<img src=\"images/spark_transformations_1.png\" style=\"width:30%\"/>\n",
    "\n",
    "Narrow Transformations are those where each input partition will contribute to only one output partition\n",
    "\n",
    "\n",
    "<img src=\"images/spark_transformations_2.png\" style=\"width:30%\"/>\n",
    "\n",
    "\n",
    "Wide Transformations are those where each input partition will contribute to several output partitions\n",
    "\n",
    "Spark will not act on transformations until we call an **action**\n",
    "\n",
    "#### Actions\n",
    "\n",
    "Transformations allow us to build up our logical transformation plan. To trigger the computation, we run an action. \n",
    "An action instructs Spark to compute a result from a series of transformations. \n",
    "\n",
    "The simplest action is **count** which gives us the total number of records in the DataFrame, but will see others, like **Take**\n",
    "\n",
    "\n",
    "\n",
    "<img src=\"images/spark_take.png\" style=\"width:50%\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "<img src=\"images/spark_word_count.png\" style=\"width:50%\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summarizing\n",
    "\n",
    "\n",
    "<img src=\"images/spark_summary.png\" style=\"width:50%\"/>\n",
    "\n",
    "---\n",
    "### Google Search  \n",
    "\n",
    "<img src=\"images/apache_spark.png\" style=\"width:50%\"/>\n",
    "$$blue=Spark$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TensorFlow\n",
    "\n",
    "<img src=\"images/Tensor_flow_logo.jpeg\"/>\n",
    "\n",
    "\n",
    "TensorFlow is an open-source system developed initially at Google to support machine-learning applications. Like Spark, TensorFlow provides a programming interface in which one writes a sequence of steps. \n",
    "\n",
    "TensorFlow uses tensors; a tensor is simply a multidimensional matrix (like Numpy arrays)\n",
    "\n",
    "TensorFlow allows to have bigger than 2 Dimensions matrices, like 5 by 5 by 5 (like a cube) -  (Numpy also allows it)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$A =  [[[1., 2.], [3., 4.]], [[5., 6.], [7., 8.]]]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make a matrix product we can use\n",
    "\n",
    "```C = tensorflow.matmul(A,B)```\n",
    "\n",
    "in numpy we could use...\n",
    "\n",
    "```python\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "C = np.matmul(A,B)\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recursive Extensions to MapReduce\n",
    "\n",
    "#### 1. Iterated MapReduce: \n",
    "Write the recursion as repeated execution of a Map- Reduce job or of a sequence of MapReduce jobs. We can then rely on the failure mechanism of the MapReduce implementation to handle failures at any step. The first example of such a system was HaLoop (see the bibliographic notes for this chapter).\n",
    "\n",
    "#### 2. The Spark Approach: \n",
    "The Spark language actually includes iterative statements, such as for-loops that allow the implementation of recursions. Here, failure management is implemented using the lazy-evaluation and lineage mechanisms of Spark. In addition, the Spark programmer has options to store intermediate states of the recursion.\n",
    "\n",
    "#### 3. Bulk-Synchronous Systems: \n",
    "These systems use a graph-based model of computation. \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
